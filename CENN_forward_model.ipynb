{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4639741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/5] Loading dataset…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e6f85bb864268a5c0550a8ab28dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading CSV files:   0%|          | 0/260 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> 20800 rows\n",
      "[2/5] Grouped train/test split (80/20) by unique (C,T)\n",
      "[4/5] Training PINN…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656238e9480e48dea95a4a46cd3de19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/5] Evaluating… (grouped split)\n",
      "   Test metrics: {'R2_Z_real': 0.999918909851281, 'MAE_Z_real': 0.27602801049562403, 'RMSE_Z_real': 0.45518183985156613, 'R2_Z_imag_neg': 0.9996785797017844, 'MAE_Z_imag_neg': 0.09053714159454425, 'RMSE_Z_imag_neg': 0.1825440852910845, 'R2_mean': 0.9997987447765326, 'MAE_mean': 0.18328257604508413, 'RMSE_mean': 0.3188629625713253}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3220a50fa24792af0ea57bd2147d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading CSV files:   0%|          | 0/260 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation rows: 20800  ->  metrics: {'R2_Z_real': 0.9999797372742756, 'MAE_Z_real': 0.08189447136066935, 'RMSE_Z_real': 0.2098234625677146, 'R2_Z_imag_neg': 0.9999182766237888, 'MAE_Z_imag_neg': 0.037848139273767414, 'RMSE_Z_imag_neg': 0.09083007713731185, 'R2_mean': 0.9999490069490322, 'MAE_mean': 0.05987130531721838, 'RMSE_mean': 0.15032676985251323}\n",
      "Saved θ(C,T) heatmaps -> /Users/hosseinostovar/Desktop/BACKUP/Data_H2SO4_NPG/data/Single_frequencies_whole_spectrum/PINN_report/theta_heatmaps\n",
      "Saved θ(C,T) grid CSV -> /Users/hosseinostovar/Desktop/BACKUP/Data_H2SO4_NPG/data/Single_frequencies_whole_spectrum/PINN_report/theta_heatmaps_csv/theta_grid_long.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1113"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================================================================\n",
    "#  PINN for EIS — Non-sweeping, TL physics\n",
    "#  Model: Z(ω) = Rs + Zarc(Rp, Y0, n0) + Z_TL(r, y0, n1, L)\n",
    "#  - Reads one folder of CSVs (parses C,T from names)\n",
    "#  - Flexible header resolver for Frequency / Z' / −Z''\n",
    "#  - Options: target normalization, loss rebalancing (MSE/Huber + weights)\n",
    "#  - Physics priors: Arrhenius (Rs,Rp), θ-invariance across f, monotonic priors\n",
    "#  - Optional teacher priors for TL parameters\n",
    "#  - Saves full report in output_dir (plots, CSVs, model)\n",
    "# =======================================================================\n",
    "\n",
    "import os, re, json, math, random, warnings, gc, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 130\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# -------------------\n",
    "# Config (EDIT PATHS)\n",
    "# -------------------\n",
    "CONFIG: Dict = {\n",
    "    \"paths\": {\n",
    "        # <<< EDIT THESE >>>\n",
    "        \"input_root\": \"/Users/hosseinostovar/Desktop/BACKUP/Data_H2SO4_NPG/data/Single_frequencies_whole_spectrum/data\",              # folder with many CSVs\n",
    "        \"fitparams_root\": None,                                    # optional: folder with EIS_FitParams_*mM_*C.csv\n",
    "        \"output_dir\": \"/Users/hosseinostovar/Desktop/BACKUP/Data_H2SO4_NPG/data/Single_frequencies_whole_spectrum/PINN_report\",            # where results go\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"column_aliases\": {\n",
    "            \"frequency\": [\"Frequency (Hz)\", \"frequency (hz)\", \"freq (hz)\", \"frequency\", \"f (hz)\", \"f_hz\", \"f\"],\n",
    "            \"z_real\":    [\"Z' (Ω)\", \"Z' (ohm)\", \"z' (ω)\", \"z_real (Ω)\", \"zreal (Ω)\", \"re(z) (Ω)\", \"re(z)\", \"z_real\", \"zre\"],\n",
    "            \"z_imag_neg\":[\"-Z'' (Ω)\", \"-Z'' (ohm)\", \"-z'' (ω)\", \"-z_imag (Ω)\", \"-im(z) (Ω)\", \"-imag (Ω)\", \"-z_imag\", \"-imag\", \"-zim\"],\n",
    "            \"z_imag_pos\":[\"Z'' (Ω)\", \"Z'' (ohm)\", \"z'' (ω)\", \"z_imag (Ω)\", \"im(z) (Ω)\", \"imag (Ω)\", \"z_imag\", \"imag\", \"zim\"],\n",
    "        },\n",
    "        # Legacy fallbacks (used only if aliases fail)\n",
    "        \"col_frequency\": \"Frequency (Hz)\",\n",
    "        \"col_z_real\":    \"Z' (Ω)\",\n",
    "        \"col_z_imag_neg\":\"-Z'' (Ω)\",\n",
    "\n",
    "        \"frequency_filter_hz\": None,         # None = use all f; or set single Hz to train single-frequency PINN\n",
    "        \"accept_nested_summary\": True,       # allow */<conc>/*/<temp>/*/single_frequency_summary.csv\n",
    "        \"accept_flat_collected\": True,       # allow arbitrary CSVs if columns match\n",
    "        \"read_csv_kwargs\": {\"encoding\": \"utf-8\"},\n",
    "\n",
    "        # Optional training-domain filter\n",
    "        \"train_filters\": {\"conc_mM_min\": 5.0, \"conc_mM_max\": 20.0, \"temp_C_min\": None, \"temp_C_max\": None},\n",
    "    },\n",
    "    \"targets\": {\"target_columns\": [\"Z_real\", \"Z_imag_neg\"]},\n",
    "    \"split\": {\"test_size\": 0.2, \"random_state\": 42, \"group_round_C\": 0, \"group_round_T\": 0},\n",
    "    \"progress\": {\"enable_bars\": True, \"show_file_scan\": True},\n",
    "    \"plots\": {\"make_learning_curve\": False, \"learning_curve_train_sizes\": [0.3, 0.6, 1.0], \"show_inline\": False},\n",
    "\n",
    "    # ------------------- PINN knobs -------------------\n",
    "    \"pinn\": {\n",
    "        \"teacher\": {\n",
    "            \"use\": False,                               # set True + provide fitparams_root for TL teacher priors\n",
    "            \"filename_glob\": \"EIS_FitParams_*mM_*C.csv\",\n",
    "            \"weight\": 0.15,\n",
    "            # Log these teacher params (positive-only); do NOT log n0/n1\n",
    "            \"log_params\": [\"Rs\",\"Rp\",\"Y0\",\"r\",\"y0\",\"L\"],\n",
    "            \"param_col\": \"parameter\",\n",
    "            \"value_col\": \"value\",\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"epochs\": 5000, \"lr\": 2e-4, \"width\": 256, \"depth\": 4,\n",
    "            \"weight_decay\": 1e-6, \"batch_size\": 512, \"verbose\": 1,\n",
    "            \"arrhenius_reg\": 1e-4, \"smooth_reg\": 1e-5, \"seed\": 42, \"device\": \"auto\",\n",
    "\n",
    "            # NEW: target normalization\n",
    "            \"targets_norm_enabled\": False,          # True/False\n",
    "            \"targets_norm_method\":  \"standard\",     # \"standard\" | \"minmax\"\n",
    "            \"targets_norm_clip\":    True,           # for minmax, clip back to [min,max] on inverse\n",
    "\n",
    "            # NEW: loss rebalancing / robust\n",
    "            \"loss_mode\":   \"mse\",                   # \"mse\" | \"huber\"\n",
    "            \"huber_delta\": 1.0,                     # δ for Huber\n",
    "            \"loss_weights\": {\"Z_real\": 1.0, \"Z_imag_neg\": 1.0},  # per-target weights\n",
    "\n",
    "            \"extra\": {\n",
    "                \"consistency_w\": 1e-3, \"group_round\": 1e-6,\n",
    "                # choose monotonicities if you know trends; default neutral except Rs:-1 vs T\n",
    "                \"mono_wC\": 0.0, \"mono_wT\": 0.0,\n",
    "                \"mono_sign_C\": {\"Rs\":0, \"Rp\":0, \"Y0\":0, \"n0\":0, \"r\":0, \"y0\":0, \"n1\":0, \"L\":0},\n",
    "                \"mono_sign_T\": {\"Rs\":-1, \"Rp\":0, \"Y0\":0, \"n0\":0, \"r\":0, \"y0\":0, \"n1\":0, \"L\":0},\n",
    "                \"teacher_nn\": True, \"teacher_nn_sigma\": 0.75\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"extrapolation\": {\"range_conc_mM\": (5.0, 20.0), \"tag\": \"conc5_20\"},\n",
    "    \"heatmap\": {\"C_min\": 5.0, \"C_max\": 20.0, \"C_points\": 64, \"T_min\": 26.0, \"T_max\": 50.0, \"T_points\": 64}\n",
    "}\n",
    "\n",
    "# ----------------------\n",
    "# Header resolution + C/T parsing\n",
    "# ----------------------\n",
    "CONC_RE = re.compile(r'(?P<val>[-+]?[0-9]*\\.?[0-9]+)\\s*(?P<unit>mM|M|uM|µM)', re.IGNORECASE)\n",
    "TEMP_RE = re.compile(r'(?P<val>[-+]?[0-9]*\\.?[0-9]+)\\s*[cC]')\n",
    "UNIT_SCALE = {\"M\": 1000.0, \"mM\": 1.0, \"uM\": 0.001, \"µM\": 0.001}\n",
    "\n",
    "def parse_concentration_to_mM(text: str) -> float:\n",
    "    if not isinstance(text, str): return np.nan\n",
    "    m = CONC_RE.search(text)\n",
    "    return float(m.group(\"val\")) * UNIT_SCALE.get(m.group(\"unit\"), 1.0) if m else np.nan\n",
    "\n",
    "def parse_temperature_to_C(text: str) -> float:\n",
    "    if not isinstance(text, str): return np.nan\n",
    "    m = TEMP_RE.search(text)\n",
    "    return float(m.group(\"val\")) if m else np.nan\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\_\\(\\)\\[\\]\\{\\}\\|'\\\"°Ωohmω]\", \"\", s.strip().lower())\n",
    "\n",
    "def _find_col_by_aliases(df: pd.DataFrame, aliases: List[str]) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    low_map = {c.lower(): c for c in cols}\n",
    "    for a in aliases:\n",
    "        if a.lower() in low_map: return low_map[a.lower()]\n",
    "    norm_map = {_normalize(c): c for c in cols}\n",
    "    for a in aliases:\n",
    "        na = _normalize(a)\n",
    "        if na in norm_map: return norm_map[na]\n",
    "    return None\n",
    "\n",
    "def resolve_eis_columns(df: pd.DataFrame) -> tuple[str, str, pd.Series]:\n",
    "    al = CONFIG[\"data\"][\"column_aliases\"]\n",
    "    f_col  = _find_col_by_aliases(df, al[\"frequency\"])\n",
    "    zr_col = _find_col_by_aliases(df, al[\"z_real\"])\n",
    "    zi_neg = _find_col_by_aliases(df, al[\"z_imag_neg\"])\n",
    "    zi_pos = _find_col_by_aliases(df, al[\"z_imag_pos\"])\n",
    "    if f_col  is None and CONFIG[\"data\"][\"col_frequency\"]   in df.columns: f_col  = CONFIG[\"data\"][\"col_frequency\"]\n",
    "    if zr_col is None and CONFIG[\"data\"][\"col_z_real\"]      in df.columns: zr_col = CONFIG[\"data\"][\"col_z_real\"]\n",
    "    if zi_neg is None and CONFIG[\"data\"][\"col_z_imag_neg\"]  in df.columns: zi_neg = CONFIG[\"data\"][\"col_z_imag_neg\"]\n",
    "    if f_col is None or zr_col is None or (zi_neg is None and zi_pos is None):\n",
    "        raise KeyError(f\"Could not resolve columns. Found={list(df.columns)}\")\n",
    "    if zi_neg is not None: zi_series = pd.to_numeric(df[zi_neg], errors=\"coerce\")\n",
    "    else:                  zi_series = -pd.to_numeric(df[zi_pos], errors=\"coerce\")\n",
    "    return f_col, zr_col, zi_series\n",
    "\n",
    "# ----------------------\n",
    "# File discovery (flat + nested)\n",
    "# ----------------------\n",
    "def discover_files(root: Path) -> List[Tuple[Path, str, str]]:\n",
    "    found = []\n",
    "    if CONFIG[\"data\"][\"accept_nested_summary\"]:\n",
    "        for p in root.rglob(\"single_frequency_summary.csv\"):\n",
    "            try:\n",
    "                temperature_str = p.parent.name\n",
    "                concentration_str = p.parent.parent.name\n",
    "                found.append((p, concentration_str, temperature_str))\n",
    "            except Exception:\n",
    "                continue\n",
    "    if CONFIG[\"data\"][\"accept_flat_collected\"]:\n",
    "        for p in root.rglob(\"*.csv\"):\n",
    "            name = p.name.lower()\n",
    "            if name == \"single_frequency_summary.csv\": continue\n",
    "            if any(tok in name for tok in [\"metrics\", \"predictions_\", \"theta_grid\", \"model_report\",\n",
    "                                           \"parity\", \"residual\", \"errhist\", \"learning\", \"training\",\n",
    "                                           \"compiled_dataset\", \"rows_with_split\"]):\n",
    "                continue\n",
    "            found.append((p, p.name, p.name))\n",
    "    uniq, seen = [], set()\n",
    "    for item in found:\n",
    "        key = str(item[0])\n",
    "        if key not in seen:\n",
    "            seen.add(key); uniq.append(item)\n",
    "    return uniq\n",
    "\n",
    "def load_one_csv(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, **CONFIG[\"data\"][\"read_csv_kwargs\"])\n",
    "\n",
    "def build_dataset(root: Path) -> pd.DataFrame:\n",
    "    files = discover_files(root)\n",
    "    if not files: raise FileNotFoundError(f\"No CSV files found under: {root}\")\n",
    "    rows = []\n",
    "    fsel = CONFIG[\"data\"][\"frequency_filter_hz\"]\n",
    "    it = tqdm(files, desc=\"Reading CSV files\", unit=\"file\") if CONFIG[\"progress\"][\"show_file_scan\"] else files\n",
    "    for csv_path, conc_hint, temp_hint in it:\n",
    "        try:\n",
    "            df = load_one_csv(csv_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "        try:\n",
    "            f_col, zr_col, zi_neg_series = resolve_eis_columns(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "        tmp = pd.DataFrame({\n",
    "            \"frequency_Hz\": pd.to_numeric(df[f_col], errors=\"coerce\"),\n",
    "            \"Z_real\":       pd.to_numeric(df[zr_col], errors=\"coerce\"),\n",
    "            \"Z_imag_neg\":   pd.to_numeric(zi_neg_series, errors=\"coerce\"),\n",
    "        })\n",
    "        if fsel is not None:\n",
    "            tmp = tmp.loc[np.isclose(tmp[\"frequency_Hz\"].astype(float), float(fsel))]\n",
    "        if tmp.empty: continue\n",
    "        c_val = parse_concentration_to_mM(str(csv_path))\n",
    "        t_val = parse_temperature_to_C(str(csv_path))\n",
    "        if np.isnan(c_val): c_val = parse_concentration_to_mM(conc_hint) or parse_concentration_to_mM(csv_path.parent.name)\n",
    "        if np.isnan(t_val): t_val = parse_temperature_to_C(temp_hint)    or parse_temperature_to_C(csv_path.parent.name)\n",
    "        tmp[\"concentration_mM\"] = c_val\n",
    "        tmp[\"temperature_C\"]    = t_val\n",
    "        tmp[\"source_file\"]      = str(csv_path)\n",
    "        rows.append(tmp)\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"Discovered files, but none had required columns / valid C,T.\")\n",
    "    data = pd.concat(rows, ignore_index=True)\n",
    "    for c in [\"concentration_mM\",\"temperature_C\",\"frequency_Hz\",\"Z_real\",\"Z_imag_neg\"]:\n",
    "        data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"concentration_mM\",\"temperature_C\",\"frequency_Hz\",\"Z_real\",\"Z_imag_neg\"])\n",
    "    fcfg = CONFIG[\"data\"][\"train_filters\"]\n",
    "    if any(v is not None for v in fcfg.values()):\n",
    "        m = pd.Series(True, index=data.index)\n",
    "        if fcfg[\"conc_mM_min\"] is not None: m &= data[\"concentration_mM\"] >= float(fcfg[\"conc_mM_min\"])\n",
    "        if fcfg[\"conc_mM_max\"] is not None: m &= data[\"concentration_mM\"] <= float(fcfg[\"conc_mM_max\"])\n",
    "        if fcfg[\"temp_C_min\"]  is not None: m &= data[\"temperature_C\"]    >= float(fcfg[\"temp_C_min\"])\n",
    "        if fcfg[\"temp_C_max\"]  is not None: m &= data[\"temperature_C\"]    <= float(fcfg[\"temp_C_max\"])\n",
    "        data = data.loc[m].copy()\n",
    "    return data\n",
    "\n",
    "# --------------------------\n",
    "# Teacher parameter loader (TL names + aliases, optional)\n",
    "# --------------------------\n",
    "_C_RE = re.compile(r'EIS_FitParams_(?P<c>[-+]?[0-9]*\\.?[0-9]+)mM_(?P<t>[-+]?[0-9]*\\.?[0-9]+)C', re.I)\n",
    "\n",
    "_TEACHER_KEY_ALIASES = {\n",
    "    \"Rs\":\"Rs\",\"Rs (Ω)\":\"Rs\",\n",
    "    \"Rp\":\"Rp\",\"Rp (Ω)\":\"Rp\",\n",
    "    \"Y0\":\"Y0\",\"Y0_ZARC\":\"Y0\",\"Y0_ZARC (Ω^-1 s^n0)\":\"Y0\",\"Y0 (Ω^-1 s^n0)\":\"Y0\",\n",
    "    \"n0\":\"n0\",\"n0 (-)\":\"n0\",\n",
    "    \"r\":\"r\",\"r_line\":\"r\",\"r_line (Ω/len)\":\"r\",\"r (Ω/len)\":\"r\",\n",
    "    \"y0\":\"y0\",\"y0_line\":\"y0\",\"y0_line (Ω^-1 s^n1 /len)\":\"y0\",\"y0 (Ω^-1 s^n1/len)\":\"y0\",\n",
    "    \"n1\":\"n1\",\"n1 (-)\":\"n1\",\n",
    "    \"L\":\"L\",\"L (len)\":\"L\"\n",
    "}\n",
    "def _normalize_teacher_key(k: str) -> str:\n",
    "    k = str(k).strip()\n",
    "    return _TEACHER_KEY_ALIASES.get(k, k)\n",
    "\n",
    "def load_teacher_param_grid(root_dir: str, filename_glob: str, param_col=\"parameter\", value_col=\"value\") -> pd.DataFrame:\n",
    "    if not root_dir: return pd.DataFrame()\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists(): return pd.DataFrame()\n",
    "    rows = []\n",
    "    for fp in root.glob(filename_glob):\n",
    "        m = _C_RE.search(fp.stem)\n",
    "        if not m: continue\n",
    "        c = float(m.group(\"c\")); t = float(m.group(\"t\"))\n",
    "        try:\n",
    "            dfp = pd.read_csv(fp)\n",
    "        except Exception:\n",
    "            try: dfp = pd.read_excel(fp)\n",
    "            except Exception: continue\n",
    "        cols = [str(x).strip() for x in dfp.columns]\n",
    "        dfp.columns = cols\n",
    "        if param_col not in cols or value_col not in cols:\n",
    "            param_col, value_col = cols[0], cols[1]\n",
    "        dmap = { _normalize_teacher_key(r[param_col]) : float(r[value_col]) for _, r in dfp.iterrows() }\n",
    "        row = {\"concentration_mM\": c, \"temperature_C\": t}\n",
    "        for k in [\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]:\n",
    "            row[k] = dmap.get(k, np.nan)\n",
    "        rows.append(row)\n",
    "    grid = pd.DataFrame(rows)\n",
    "    for c in [\"concentration_mM\",\"temperature_C\",\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]:\n",
    "        if c in grid.columns: grid[c] = pd.to_numeric(grid[c], errors=\"coerce\")\n",
    "    return grid.dropna(subset=[\"concentration_mM\",\"temperature_C\"], how=\"any\")\n",
    "\n",
    "# --------------------------\n",
    "# Metrics + plotting helpers\n",
    "# --------------------------\n",
    "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray, names: List[str]) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    r2s, maes, rmses = [], [], []\n",
    "    for j, name in enumerate(names):\n",
    "        r2 = r2_score(y_true[:, j], y_pred[:, j])\n",
    "        mae = mean_absolute_error(y_true[:, j], y_pred[:, j])\n",
    "        rmse = math.sqrt(mean_squared_error(y_true[:, j], y_pred[:, j]))\n",
    "        out[f\"R2_{name}\"] = r2; out[f\"MAE_{name}\"] = mae; out[f\"RMSE_{name}\"] = rmse\n",
    "        r2s.append(r2); maes.append(mae); rmses.append(rmse)\n",
    "    out[\"R2_mean\"] = float(np.mean(r2s)); out[\"MAE_mean\"] = float(np.mean(maes)); out[\"RMSE_mean\"] = float(np.mean(rmses))\n",
    "    return out\n",
    "\n",
    "def _maybe_show():\n",
    "    if CONFIG[\"plots\"][\"show_inline\"]: plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def parity_plot_and_csv(y_true, y_pred, target_name, img_path: Path, csv_path: Path):\n",
    "    pd.DataFrame({\"y_true\": y_true.ravel(), \"y_pred\": y_pred.ravel()}).to_csv(csv_path, index=False)\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=12, alpha=0.7)\n",
    "    low = min(float(np.min(y_true)), float(np.min(y_pred))); high = max(float(np.max(y_true)), float(np.max(y_pred)))\n",
    "    plt.plot([low, high], [low, high], linestyle=\"--\")\n",
    "    plt.xlabel(f\"True {target_name}\"); plt.ylabel(f\"Pred {target_name}\")\n",
    "    plt.title(f\"Parity: {target_name}\")\n",
    "    plt.tight_layout(); plt.savefig(img_path, dpi=180); _maybe_show()\n",
    "\n",
    "def residual_plot_and_csv(y_true, y_pred, target_name, img_path: Path, csv_path: Path):\n",
    "    residual = (y_pred - y_true).ravel()\n",
    "    pd.DataFrame({\"y_pred\": y_pred.ravel(), \"residual\": residual}).to_csv(csv_path, index=False)\n",
    "    plt.figure()\n",
    "    plt.scatter(y_pred, residual, s=12, alpha=0.7)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.xlabel(f\"Pred {target_name}\"); plt.ylabel(\"Residual (Pred - True)\")\n",
    "    plt.title(f\"Residuals: {target_name}\")\n",
    "    plt.tight_layout(); plt.savefig(img_path, dpi=180); _maybe_show()\n",
    "\n",
    "def _gauss_pdf(x, m, s):\n",
    "    if s <= 0: return np.zeros_like(x)\n",
    "    return (1.0 / (s * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - m)/s)**2)\n",
    "\n",
    "def error_hist_plot_and_csv(y_true, y_pred, target_name, img_path: Path, bins: int, hist_csv_path: Path, fit_csv_path: Path):\n",
    "    err = (y_pred - y_true).ravel()\n",
    "    counts, edges = np.histogram(err, bins=bins)\n",
    "    centers = 0.5*(edges[:-1] + edges[1:])\n",
    "    bin_w = float(edges[1]-edges[0]) if len(edges) > 1 else 1.0\n",
    "    mode_center = float(centers[np.argmax(counts)])\n",
    "    med = float(np.median(err)); mad = float(np.median(np.abs(err - med)))\n",
    "    sigma_rob = 1.4826*mad if mad > 0 else float(np.std(err, ddof=1))\n",
    "    mean_bias = float(np.mean(err)); sigma_std = float(np.std(err, ddof=1)) if err.size > 1 else 0.0\n",
    "    ci_low = mode_center - 1.96*sigma_rob; ci_high = mode_center + 1.96*sigma_rob\n",
    "    pd.DataFrame({\"bin_left\": edges[:-1], \"bin_right\": edges[1:], \"bin_center\": centers, \"count\": counts}).to_csv(hist_csv_path, index=False)\n",
    "    pd.DataFrame([{\"mu_mode\": mode_center, \"sigma_robust\": sigma_rob, \"mean_bias\": mean_bias, \"sigma_std\": sigma_std,\n",
    "                   \"ci95_low\": ci_low, \"ci95_high\": ci_high, \"n\": int(len(err)), \"bin_width\": bin_w}]).to_csv(fit_csv_path, index=False)\n",
    "    xg = np.linspace(edges[0], edges[-1], 600)\n",
    "    gauss_counts = len(err) * bin_w * _gauss_pdf(xg, mode_center, sigma_rob)\n",
    "    plt.figure(); plt.hist(err, bins=bins); plt.plot(xg, gauss_counts, linewidth=2)\n",
    "    plt.xlabel(\"Error (Pred - True)\"); plt.ylabel(\"Count\"); plt.title(f\"Error Distribution: {target_name}\")\n",
    "    plt.tight_layout(); plt.savefig(img_path, dpi=180); _maybe_show()\n",
    "\n",
    "# --------------------------\n",
    "# Physics core — TL model in torch (complex)\n",
    "# --------------------------\n",
    "def _j_like(x):\n",
    "    return torch.complex(torch.zeros((), dtype=x.dtype, device=x.device),\n",
    "                         torch.ones( (), dtype=x.dtype, device=x.device))\n",
    "\n",
    "def torch_coth(z, eps=1e-12):\n",
    "    sz = torch.sinh(z); cz = torch.cosh(z)\n",
    "    small = torch.abs(sz) < eps\n",
    "    out = torch.empty_like(z)\n",
    "    out[~small] = cz[~small] / sz[~small]\n",
    "    out[small] = 1.0/z[small] + z[small]/3.0   # series\n",
    "    return out\n",
    "\n",
    "def torch_zarc(Rp, Y0, n, w):\n",
    "    j = _j_like(w)\n",
    "    return 1.0 / (1.0/torch.clamp(Rp, min=1e-18) + torch.clamp(Y0, min=1e-18) * (j*w)**n)\n",
    "\n",
    "def torch_tl_impedance(r, y0, n, L, w):\n",
    "    j = _j_like(w)\n",
    "    r_ = torch.clamp(r,  min=1e-18); y0_= torch.clamp(y0, min=1e-18)\n",
    "    gamma = torch.sqrt(r_ * y0_ * (j*w)**n)\n",
    "    Z0    = torch.sqrt(r_ / (y0_ * (j*w)**n))\n",
    "    return Z0 * torch_coth(L * gamma)\n",
    "\n",
    "def torch_impedance_rs_zarc_tl(omega, Rs, Rp, Y0, n0, r, y0, n1, L):\n",
    "    Zarc = torch_zarc(Rp, Y0, n0, omega)\n",
    "    Ztl  = torch_tl_impedance(r, y0, n1, L, omega)\n",
    "    return Rs + Zarc + Ztl\n",
    "\n",
    "# --------------------------\n",
    "# Network θ(C,T) → [Rs, Rp, Y0, n0, r, y0, n1, L]\n",
    "# --------------------------\n",
    "class ThetaNet(nn.Module):\n",
    "    def __init__(self, in_dim=2, width=64, depth=3, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        layers, d = [], in_dim\n",
    "        for _ in range(depth):\n",
    "            layers += [nn.Linear(d, width, dtype=dtype), nn.ReLU()]\n",
    "            d = width\n",
    "        self.backbone = nn.Sequential(*layers) if layers else nn.Identity()\n",
    "        self.head = nn.Linear(d, 8, dtype=dtype)  # Rs, Rp, Y0, n0, r, y0, n1, L\n",
    "        self.softplus = nn.Softplus(); self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, Cn, Tn):\n",
    "        h = self.backbone(torch.stack([Cn, Tn], dim=1))\n",
    "        raw = self.head(h)\n",
    "        Rs_r, Rp_r, Y0_r, n0_r, r_r, y0_r, n1_r, L_r = torch.unbind(raw, dim=1)\n",
    "        eps = 1e-9\n",
    "        Rs  = self.softplus(Rs_r)  + eps\n",
    "        Rp  = self.softplus(Rp_r)  + eps\n",
    "        Y0  = self.softplus(Y0_r)  + eps\n",
    "        n0  = self.sigmoid(n0_r)   # (0,1)\n",
    "        r   = self.softplus(r_r)   + eps\n",
    "        y0  = self.softplus(y0_r)  + eps\n",
    "        n1  = self.sigmoid(n1_r)   # (0,1)\n",
    "        L   = self.softplus(L_r)   + eps\n",
    "        return Rs, Rp, Y0, n0, r, y0, n1, L\n",
    "\n",
    "# --------------------------\n",
    "# EEC_PINN (TL) with normalization + loss options\n",
    "# --------------------------\n",
    "class EEC_PINN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, teacher_df=None, teacher_use=False, teacher_weight=0.15, teacher_log_params=None,\n",
    "                 epochs=3500, lr=7.5e-4, width=64, depth=3, weight_decay=1e-6,\n",
    "                 batch_size=512, verbose=1, arrhenius_reg=1e-4, smooth_reg=1e-5,\n",
    "                 seed=42, device=\"auto\",\n",
    "                 consistency_w=1e-3, group_round=1e-6,\n",
    "                 mono_wC=0.0, mono_wT=0.0,\n",
    "                 mono_sign_C=None, mono_sign_T=None,\n",
    "                 teacher_nn=True, teacher_nn_sigma=0.75,\n",
    "                 # NEW:\n",
    "                 targets_norm_enabled=False, targets_norm_method=\"standard\", targets_norm_clip=True,\n",
    "                 loss_mode=\"mse\", huber_delta=1.0, loss_weights=None):\n",
    "        # teacher/prior\n",
    "        self.teacher_df = teacher_df\n",
    "        self.teacher_use = bool(teacher_use)\n",
    "        self.teacher_weight = float(teacher_weight)\n",
    "        self.teacher_log_params = teacher_log_params or [\"Rs\",\"Rp\",\"Y0\",\"r\",\"y0\",\"L\"]\n",
    "        self.teacher_nn = bool(teacher_nn); self.teacher_nn_sigma=float(teacher_nn_sigma)\n",
    "        # training\n",
    "        self.epochs=int(epochs); self.lr=float(lr); self.width=int(width); self.depth=int(depth)\n",
    "        self.weight_decay=float(weight_decay); self.batch_size=int(batch_size); self.verbose=int(verbose)\n",
    "        self.arrhenius_reg=float(arrhenius_reg); self.smooth_reg=float(smooth_reg)\n",
    "        self.seed=int(seed); self.device=device\n",
    "        # priors\n",
    "        self.consistency_w=float(consistency_w); self.group_round=float(group_round)\n",
    "        self.mono_wC=float(mono_wC); self.mono_wT=float(mono_wT)\n",
    "        self.mono_sign_C = mono_sign_C or {\"Rs\":0,\"Rp\":0,\"Y0\":0,\"n0\":0,\"r\":0,\"y0\":0,\"n1\":0,\"L\":0}\n",
    "        self.mono_sign_T = mono_sign_T or {\"Rs\":-1,\"Rp\":0,\"Y0\":0,\"n0\":0,\"r\":0,\"y0\":0,\"n1\":0,\"L\":0}\n",
    "        # targets + loss\n",
    "        self.targets_norm_enabled=bool(targets_norm_enabled)\n",
    "        self.targets_norm_method=str(targets_norm_method)\n",
    "        self.targets_norm_clip=bool(targets_norm_clip)\n",
    "        self.loss_mode=str(loss_mode)\n",
    "        self.huber_delta=float(huber_delta)\n",
    "        self.loss_weights = loss_weights or {\"Z_real\": 1.0, \"Z_imag_neg\": 1.0}\n",
    "\n",
    "        # internals\n",
    "        self._dtype=torch.float64; self._xmu=None; self._xstd=None; self._net=None\n",
    "        self._teacher_map={}; self._teacher_CT=None; self._teacher_TH=None\n",
    "        self._y_norm = {\"enabled\": False}\n",
    "        self.loss_history = []\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"teacher_df\": self.teacher_df, \"teacher_use\": self.teacher_use, \"teacher_weight\": self.teacher_weight,\n",
    "            \"teacher_log_params\": self.teacher_log_params, \"teacher_nn\": self.teacher_nn, \"teacher_nn_sigma\": self.teacher_nn_sigma,\n",
    "            \"epochs\": self.epochs, \"lr\": self.lr, \"width\": self.width, \"depth\": self.depth,\n",
    "            \"weight_decay\": self.weight_decay, \"batch_size\": self.batch_size, \"verbose\": self.verbose,\n",
    "            \"arrhenius_reg\": self.arrhenius_reg, \"smooth_reg\": self.smooth_reg,\n",
    "            \"seed\": self.seed, \"device\": self.device,\n",
    "            \"consistency_w\": self.consistency_w, \"group_round\": self.group_round,\n",
    "            \"mono_wC\": self.mono_wC, \"mono_wT\": self.mono_wT,\n",
    "            \"mono_sign_C\": self.mono_sign_C, \"mono_sign_T\": self.mono_sign_T,\n",
    "            \"targets_norm_enabled\": self.targets_norm_enabled,\n",
    "            \"targets_norm_method\": self.targets_norm_method,\n",
    "            \"targets_norm_clip\": self.targets_norm_clip,\n",
    "            \"loss_mode\": self.loss_mode, \"huber_delta\": self.huber_delta,\n",
    "            \"loss_weights\": self.loss_weights\n",
    "        }\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items(): setattr(self, k, v)\n",
    "        return self\n",
    "\n",
    "    def _dev(self):\n",
    "        return torch.device(\"cuda\" if (self.device==\"auto\" and torch.cuda.is_available()) else (self.device if self.device!=\"auto\" else \"cpu\"))\n",
    "    def _std_CT(self, C, T):\n",
    "        X = np.stack([C,T], axis=1)\n",
    "        if self._xmu is None:\n",
    "            self._xmu = X.mean(axis=0); self._xstd = X.std(axis=0) + 1e-12\n",
    "        Xn = (X - self._xmu) / self._xstd\n",
    "        return Xn[:,0], Xn[:,1]\n",
    "\n",
    "    # --- target normalization (Z', -Z'') ---\n",
    "    def _fit_y_norm(self, y):\n",
    "        if not self.targets_norm_enabled:\n",
    "            self._y_norm = {\"enabled\": False}; return\n",
    "        y = np.asarray(y, float)\n",
    "        mu = y.mean(axis=0); std = y.std(axis=0) + 1e-12\n",
    "        y_min = y.min(axis=0); y_max = y.max(axis=0)\n",
    "        self._y_norm = {\"enabled\": True, \"method\": self.targets_norm_method,\n",
    "                        \"mu\": mu, \"std\": std, \"min\": y_min, \"max\": y_max}\n",
    "    def _y_transform(self, y):\n",
    "        if not self._y_norm.get(\"enabled\", False): return y\n",
    "        if self._y_norm[\"method\"] == \"standard\":\n",
    "            return (y - self._y_norm[\"mu\"]) / self._y_norm[\"std\"]\n",
    "        elif self._y_norm[\"method\"] == \"minmax\":\n",
    "            rng = (self._y_norm[\"max\"] - self._y_norm[\"min\"]); rng[rng==0] = 1.0\n",
    "            return (y - self._y_norm[\"min\"]) / rng\n",
    "        return y\n",
    "    def _y_inverse(self, y_norm):\n",
    "        if not self._y_norm.get(\"enabled\", False): return y_norm\n",
    "        if self._y_norm[\"method\"] == \"standard\":\n",
    "            return y_norm * self._y_norm[\"std\"] + self._y_norm[\"mu\"]\n",
    "        elif self._y_norm[\"method\"] == \"minmax\":\n",
    "            rng = (self._y_norm[\"max\"] - self._y_norm[\"min\"]); rng[rng==0] = 1.0\n",
    "            y = y_norm * rng + self._y_norm[\"min\"]\n",
    "            if self.targets_norm_clip:\n",
    "                y = np.clip(y, self._y_norm[\"min\"], self._y_norm[\"max\"])\n",
    "            return y\n",
    "        return y_norm\n",
    "\n",
    "    def _build_teacher_map(self):\n",
    "        self._teacher_map = {}\n",
    "        if not (self.teacher_use and (self.teacher_df is not None) and (not self.teacher_df.empty)):\n",
    "            return\n",
    "        def keyify(c,t): return (round(float(c),6), round(float(t),6))\n",
    "        for _, r in self.teacher_df.iterrows():\n",
    "            th = [r.get(k, np.nan) for k in [\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]]\n",
    "            if not np.isnan(th).any():\n",
    "                self._teacher_map[keyify(r[\"concentration_mM\"], r[\"temperature_C\"])] = th\n",
    "    def _theta_teacher_for(self, C,T):\n",
    "        return self._teacher_map.get((round(float(C),6), round(float(T),6)), None)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        torch.manual_seed(self.seed); np.random.seed(self.seed); random.seed(self.seed)\n",
    "        self.loss_history = []\n",
    "        dev=self._dev(); dtype=self._dtype\n",
    "        C, T, f = X[:,0].astype(float), X[:,1].astype(float), X[:,2].astype(float)\n",
    "        Zr, Zim = y[:,0].astype(float), y[:,1].astype(float)\n",
    "        Cn, Tn = self._std_CT(C, T); w = 2*np.pi*f\n",
    "\n",
    "        # fit target normalization on training y\n",
    "        Y_train = np.column_stack([Zr, Zim]); self._fit_y_norm(Y_train)\n",
    "\n",
    "        C_t=torch.tensor(Cn,dtype=dtype,device=dev); T_t=torch.tensor(Tn,dtype=dtype,device=dev)\n",
    "        w_t=torch.tensor(w,dtype=dtype,device=dev)\n",
    "        Zr_t=torch.tensor(Zr,dtype=dtype,device=dev); Zim_t=torch.tensor(Zim,dtype=dtype,device=dev)\n",
    "\n",
    "        # teacher buffers\n",
    "        if self.teacher_use and (self.teacher_df is not None):\n",
    "            self._build_teacher_map()\n",
    "            if self.teacher_nn and (not self.teacher_df.empty):\n",
    "                Ct = self.teacher_df[[\"concentration_mM\",\"temperature_C\"]].to_numpy(float)\n",
    "                Ctn, Ttn = self._std_CT(Ct[:,0], Ct[:,1])\n",
    "                self._teacher_CT = torch.tensor(np.column_stack([Ctn,Ttn]), dtype=dtype, device=dev)\n",
    "                self._teacher_TH = torch.tensor(self.teacher_df[[\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]].to_numpy(float),\n",
    "                                                dtype=dtype, device=dev)\n",
    "\n",
    "        self._net = ThetaNet(in_dim=2, width=self.width, depth=self.depth, dtype=dtype).to(dev)\n",
    "        opt = optim.Adam(self._net.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        def _weighted_loss(diff_re, diff_im):\n",
    "            if self.loss_mode == \"huber\":\n",
    "                delta = self.huber_delta\n",
    "                huber_re = torch.where(diff_re.abs() <= delta, 0.5*diff_re.pow(2), delta*(diff_re.abs()-0.5*delta))\n",
    "                huber_im = torch.where(diff_im.abs() <= delta, 0.5*diff_im.pow(2), delta*(diff_im.abs()-0.5*delta))\n",
    "                return self.loss_weights[\"Z_real\"]*huber_re.mean() + self.loss_weights[\"Z_imag_neg\"]*huber_im.mean()\n",
    "            else:  # mse\n",
    "                return self.loss_weights[\"Z_real\"]*(diff_re.pow(2).mean()) + self.loss_weights[\"Z_imag_neg\"]*(diff_im.pow(2).mean())\n",
    "\n",
    "        N=len(C); nb=max(1, N//self.batch_size)\n",
    "        loop=trange(self.epochs, disable=(self.verbose==0))\n",
    "        for _ in loop:\n",
    "            perm=torch.randperm(N, device=dev); epoch_loss=0.0\n",
    "            for b in range(nb):\n",
    "                sel=perm[b*self.batch_size:(b+1)*self.batch_size]\n",
    "                Cb,Tb,wb = C_t[sel],T_t[sel],w_t[sel]\n",
    "                Zr_b,Zim_b = Zr_t[sel], Zim_t[sel]\n",
    "\n",
    "                Rs,Rp,Y0,n0,r,y0,n1,L = self._net(Cb,Tb)\n",
    "                Zc = torch_impedance_rs_zarc_tl(wb, Rs,Rp,Y0,n0,r,y0,n1,L)\n",
    "                yhat_re = Zc.real; yhat_im = -Zc.imag\n",
    "\n",
    "                # targets normalization (apply on both yhat & ytrue)\n",
    "                if self.targets_norm_enabled:\n",
    "                    yhat_np = np.column_stack([yhat_re.detach().cpu().numpy(),\n",
    "                                               yhat_im.detach().cpu().numpy()])\n",
    "                    ytrue_np= np.column_stack([Zr_b.detach().cpu().numpy(),\n",
    "                                               Zim_b.detach().cpu().numpy()])\n",
    "                    yhat_n  = self._y_transform(yhat_np); ytrue_n = self._y_transform(ytrue_np)\n",
    "                    diff_re = torch.tensor(yhat_n[:,0]-ytrue_n[:,0], dtype=dtype, device=dev)\n",
    "                    diff_im = torch.tensor(yhat_n[:,1]-ytrue_n[:,1], dtype=dtype, device=dev)\n",
    "                else:\n",
    "                    diff_re = (yhat_re - Zr_b); diff_im = (yhat_im - Zim_b)\n",
    "\n",
    "                loss = _weighted_loss(diff_re, diff_im)\n",
    "\n",
    "                # --- physics-informed extras ---\n",
    "\n",
    "                # exact teacher at same (C,T)\n",
    "                if self.teacher_use and (self.teacher_weight > 0) and len(self._teacher_map) > 0:\n",
    "                    Cbo = (Cb.cpu().numpy()*self._xstd[0] + self._xmu[0])\n",
    "                    Tbo = (Tb.cpu().numpy()*self._xstd[1] + self._xmu[1])\n",
    "                    idxs, th_list = [], []\n",
    "                    for i in range(len(Cbo)):\n",
    "                        th = self._theta_teacher_for(Cbo[i], Tbo[i])\n",
    "                        if th is not None: idxs.append(i); th_list.append(th)\n",
    "                    if idxs:\n",
    "                        targ=torch.tensor(np.array(th_list),dtype=dtype,device=dev)\n",
    "                        pred=torch.stack([Rs,Rp,Y0,n0,r,y0,n1,L],dim=1)[idxs]\n",
    "                        names=[\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]\n",
    "                        log_mask=np.array([n in self.teacher_log_params for n in names],bool)\n",
    "                        pred_s=pred.clone(); targ_s=targ.clone()\n",
    "                        for j,islog in enumerate(log_mask):\n",
    "                            if islog:\n",
    "                                pred_s[:,j]=torch.log(pred_s[:,j]+1e-12)\n",
    "                                targ_s[:,j]=torch.log(targ_s[:,j]+1e-12)\n",
    "                        loss = loss + self.teacher_weight*torch.mean((pred_s - targ_s)**2)\n",
    "\n",
    "                # nearest-teacher soft prior\n",
    "                if self.teacher_use and (self.teacher_weight > 0) and (self._teacher_CT is not None):\n",
    "                    Q = torch.stack([Cb, Tb], dim=1)\n",
    "                    d2 = torch.cdist(Q, self._teacher_CT).pow(2)\n",
    "                    wts = torch.softmax(-d2 / (2*self.teacher_nn_sigma**2), dim=1)\n",
    "                    theta_pred = torch.stack([Rs,Rp,Y0,n0,r,y0,n1,L], dim=1)\n",
    "                    theta_targ = wts @ self._teacher_TH\n",
    "                    names = [\"Rs\",\"Rp\",\"Y0\",\"n0\",\"r\",\"y0\",\"n1\",\"L\"]\n",
    "                    log_mask = torch.tensor([n in self.teacher_log_params for n in names], device=dev)\n",
    "                    pred_s = theta_pred.clone(); targ_s = theta_targ.clone()\n",
    "                    pred_s[:, log_mask] = torch.log(pred_s[:, log_mask] + 1e-12)\n",
    "                    targ_s[:, log_mask] = torch.log(targ_s[:, log_mask] + 1e-12)\n",
    "                    loss = loss + 0.5*self.teacher_weight*torch.mean((pred_s - targ_s)**2)\n",
    "\n",
    "                # θ-invariance across identical (C,T) within batch\n",
    "                if self.consistency_w > 0:\n",
    "                    with torch.no_grad():\n",
    "                        keys = torch.round(torch.stack([Cb, Tb], dim=1) / self.group_round)\n",
    "                        uniq, inv = torch.unique(keys, dim=0, return_inverse=True)\n",
    "                    theta_b = torch.stack([Rs,Rp,Y0,n0,r,y0,n1,L], dim=1)\n",
    "                    var_sum = 0.0\n",
    "                    for gid in range(uniq.size(0)):\n",
    "                        mask = (inv == gid)\n",
    "                        if mask.sum() > 1:\n",
    "                            var_sum = var_sum + theta_b[mask].var(dim=0, unbiased=False).mean()\n",
    "                    loss = loss + self.consistency_w * var_sum\n",
    "\n",
    "                # monotonic priors (optional)\n",
    "                def _mono_pen(grad, sign):  # penalize wrong sign\n",
    "                    return torch.relu((-sign) * grad).mean()\n",
    "                if (self.mono_wC > 0) or (self.mono_wT > 0):\n",
    "                    Cb_req = Cb.clone().detach().requires_grad_(self.mono_wC > 0)\n",
    "                    Tb_req = Tb.clone().detach().requires_grad_(self.mono_wT > 0)\n",
    "                    Rs2,Rp2,Y02,n02,r2,y02,n12,L2 = self._net(Cb_req, Tb_req)\n",
    "                    thetas = {\"Rs\":Rs2,\"Rp\":Rp2,\"Y0\":Y02,\"n0\":n02,\"r\":r2,\"y0\":y02,\"n1\":n12,\"L\":L2}\n",
    "                    if self.mono_wC > 0:\n",
    "                        penC = 0.0\n",
    "                        for name, th in thetas.items():\n",
    "                            sgn = self.mono_sign_C.get(name, 0)\n",
    "                            if sgn != 0:\n",
    "                                gC = torch.autograd.grad(th.sum(), Cb_req, retain_graph=True, create_graph=False)[0]\n",
    "                                penC = penC + _mono_pen(gC, sgn)\n",
    "                        loss = loss + self.mono_wC * penC\n",
    "                    if self.mono_wT > 0:\n",
    "                        penT = 0.0\n",
    "                        for name, th in thetas.items():\n",
    "                            sgn = self.mono_sign_T.get(name, 0)\n",
    "                            if sgn != 0:\n",
    "                                gT = torch.autograd.grad(th.sum(), Tb_req, retain_graph=True, create_graph=False)[0]\n",
    "                                penT = penT + _mono_pen(gT, sgn)\n",
    "                        loss = loss + self.mono_wT * penT\n",
    "\n",
    "                # Arrhenius + smoothness\n",
    "                if self.arrhenius_reg > 0:\n",
    "                    TK = (Tb * self._xstd[1] + self._xmu[1]) + 273.15\n",
    "                    invTK = 1.0 / (TK + 1e-9)\n",
    "                    def lin_resid(x, y):\n",
    "                        X1 = torch.stack([torch.ones_like(x), x], dim=1)\n",
    "                        beta, *_ = torch.linalg.lstsq(X1, y.unsqueeze(1))\n",
    "                        yhat = (X1 @ beta).squeeze(1)\n",
    "                        return torch.mean((y - yhat)**2)\n",
    "                    loss = loss + self.arrhenius_reg * (lin_resid(invTK, torch.log(Rs)) + lin_resid(invTK, torch.log(Rp)))\n",
    "                if self.smooth_reg > 0 and sel.numel() > 1:\n",
    "                    params = torch.stack([Rs,Rp,Y0,n0,r,y0,n1,L], dim=1)\n",
    "                    loss = loss + self.smooth_reg*torch.mean(params.var(dim=0))\n",
    "\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "                epoch_loss += float(loss.detach())\n",
    "            if self.verbose: loop.set_description(f\"PINN loss {epoch_loss/nb:.4g}\")\n",
    "            self.loss_history.append(float(epoch_loss/nb))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dev=self._dev(); dtype=self._dtype\n",
    "        C,T,f = X[:,0].astype(float), X[:,1].astype(float), X[:,2].astype(float)\n",
    "        Cn,Tn = self._std_CT(C,T)\n",
    "        with torch.no_grad():\n",
    "            C_t=torch.tensor(Cn,dtype=dtype,device=dev)\n",
    "            T_t=torch.tensor(Tn,dtype=dtype,device=dev)\n",
    "            w_t=torch.tensor(2*np.pi*f,dtype=dtype,device=dev)\n",
    "            Rs,Rp,Y0,n0,r,y0,n1,L = self._net(C_t,T_t)\n",
    "            Zc = torch_impedance_rs_zarc_tl(w_t, Rs,Rp,Y0,n0,r,y0,n1,L)\n",
    "            y = torch.stack([Zc.real, -Zc.imag], dim=1).cpu().numpy()\n",
    "        return self._y_inverse(y) if self.targets_norm_enabled else y\n",
    "\n",
    "    def predict_theta(self, C_grid, T_grid):\n",
    "        dev=self._dev(); dtype=self._dtype\n",
    "        C_flat=np.asarray(C_grid).ravel(); T_flat=np.asarray(T_grid).ravel()\n",
    "        Cn,Tn=self._std_CT(C_flat,T_flat)\n",
    "        with torch.no_grad():\n",
    "            C_t=torch.tensor(Cn,dtype=dtype,device=dev); T_t=torch.tensor(Tn,dtype=dtype,device=dev)\n",
    "            Rs,Rp,Y0,n0,r,y0,n1,L = self._net(C_t,T_t)\n",
    "            def R(x): return x.cpu().numpy().reshape(C_grid.shape)\n",
    "            return {\"Rs\":R(Rs),\"Rp\":R(Rp),\"Y0\":R(Y0),\"n0\":R(n0),\"r\":R(r),\"y0\":R(y0),\"n1\":R(n1),\"L\":R(L)}\n",
    "\n",
    "# --------------------------\n",
    "# Helpers: grouped split\n",
    "# --------------------------\n",
    "def make_group_ids(df: pd.DataFrame, c_col=\"concentration_mM\", t_col=\"temperature_C\") -> np.ndarray:\n",
    "    rc = float(CONFIG[\"split\"][\"group_round_C\"])\n",
    "    rt = float(CONFIG[\"split\"][\"group_round_T\"])\n",
    "    Cg = np.round(df[c_col].astype(float) / rc) * rc if rc > 0 else df[c_col].astype(float)\n",
    "    Tg = np.round(df[t_col].astype(float) / rt) * rt if rt > 0 else df[t_col].astype(float)\n",
    "    return (Cg.astype(str) + \"|\" + Tg.astype(str)).to_numpy()\n",
    "\n",
    "# --------------------------\n",
    "# Train + evaluate + save\n",
    "# --------------------------\n",
    "def train_pinn_and_report():\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"]).resolve()\n",
    "    plots_dir = out_dir / \"plots\"; plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plots_data_dir = out_dir / \"plots_data\"; plots_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sub in [\"parity\",\"residuals\",\"hist\",\"learning\",\"training\",\"split_info\"]:\n",
    "        (plots_dir/sub).mkdir(exist_ok=True); (plots_data_dir/sub).mkdir(exist_ok=True)\n",
    "\n",
    "    with open(out_dir / \"config_used.json\",\"w\",encoding=\"utf-8\") as f: json.dump(CONFIG, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"[1/5] Loading dataset…\")\n",
    "    data = build_dataset(Path(CONFIG[\"paths\"][\"input_root\"]))\n",
    "    data.to_csv(out_dir / \"compiled_dataset.csv\", index=False)\n",
    "    print(f\"   -> {len(data)} rows\")\n",
    "\n",
    "    feat_cols = [\"concentration_mM\",\"temperature_C\",\"frequency_Hz\"]\n",
    "    targ_cols = CONFIG[\"targets\"][\"target_columns\"]\n",
    "    X = data[feat_cols].values\n",
    "    y = data[targ_cols].values\n",
    "\n",
    "    print(\"[2/5] Grouped train/test split (80/20) by unique (C,T)\")\n",
    "    groups = make_group_ids(data, \"concentration_mM\", \"temperature_C\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG[\"split\"][\"test_size\"],\n",
    "                            random_state=CONFIG[\"split\"][\"random_state\"])\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # save split diagnostics\n",
    "    df_train = data.iloc[train_idx].copy(); df_train[\"split\"]=\"train\"\n",
    "    df_test  = data.iloc[test_idx ].copy(); df_test[\"split\"]=\"test\"\n",
    "    pd.concat([df_train, df_test], axis=0).to_csv(plots_data_dir / \"split_info\" / \"rows_with_split.csv\", index=False)\n",
    "    ct_train = set((round(c,6), round(t,6)) for c,t in zip(df_train.concentration_mM, df_train.temperature_C))\n",
    "    ct_test  = set((round(c,6), round(t,6)) for c,t in zip(df_test.concentration_mM,  df_test.temperature_C))\n",
    "    with open(plots_data_dir / \"split_info\" / \"ct_overlap_check.txt\",\"w\") as fh:\n",
    "        fh.write(f\"Unique (C,T) train: {len(ct_train)}\\nUnique (C,T) test:  {len(ct_test)}\\nOverlap count: {len(ct_train.intersection(ct_test))}\\n\")\n",
    "\n",
    "    teacher_df = pd.DataFrame()\n",
    "    if CONFIG[\"pinn\"][\"teacher\"][\"use\"] and CONFIG[\"paths\"][\"fitparams_root\"]:\n",
    "        print(\"[3/5] Loading teacher parameters…\")\n",
    "        teacher_df = load_teacher_param_grid(\n",
    "            CONFIG[\"paths\"][\"fitparams_root\"],\n",
    "            CONFIG[\"pinn\"][\"teacher\"][\"filename_glob\"],\n",
    "            CONFIG[\"pinn\"][\"teacher\"][\"param_col\"],\n",
    "            CONFIG[\"pinn\"][\"teacher\"][\"value_col\"],\n",
    "        )\n",
    "        if teacher_df.empty: print(\"   (no teacher files found; continuing without)\")\n",
    "\n",
    "    print(\"[4/5] Training PINN…\")\n",
    "    p = CONFIG[\"pinn\"][\"train\"]; extra = p[\"extra\"]\n",
    "    pinn = EEC_PINN(\n",
    "        # teacher\n",
    "        teacher_df=teacher_df if (CONFIG[\"pinn\"][\"teacher\"][\"use\"] and not teacher_df.empty) else None,\n",
    "        teacher_use=CONFIG[\"pinn\"][\"teacher\"][\"use\"],\n",
    "        teacher_weight=CONFIG[\"pinn\"][\"teacher\"][\"weight\"],\n",
    "        teacher_log_params=CONFIG[\"pinn\"][\"teacher\"][\"log_params\"],\n",
    "        teacher_nn=extra[\"teacher_nn\"], teacher_nn_sigma=extra[\"teacher_nn_sigma\"],\n",
    "        # training\n",
    "        epochs=p[\"epochs\"], lr=p[\"lr\"], width=p[\"width\"], depth=p[\"depth\"],\n",
    "        weight_decay=p[\"weight_decay\"], batch_size=p[\"batch_size\"], verbose=p[\"verbose\"],\n",
    "        arrhenius_reg=p[\"arrhenius_reg\"], smooth_reg=p[\"smooth_reg\"],\n",
    "        seed=p[\"seed\"], device=p.get(\"device\",\"auto\"),\n",
    "        # priors\n",
    "        consistency_w=extra[\"consistency_w\"], group_round=extra[\"group_round\"],\n",
    "        mono_wC=extra[\"mono_wC\"], mono_wT=extra[\"mono_wT\"],\n",
    "        mono_sign_C=extra[\"mono_sign_C\"], mono_sign_T=extra[\"mono_sign_T\"],\n",
    "        # NEW: normalization + loss\n",
    "        targets_norm_enabled=p[\"targets_norm_enabled\"],\n",
    "        targets_norm_method=p[\"targets_norm_method\"],\n",
    "        targets_norm_clip=p[\"targets_norm_clip\"],\n",
    "        loss_mode=p[\"loss_mode\"], huber_delta=p[\"huber_delta\"],\n",
    "        loss_weights=p[\"loss_weights\"]\n",
    "    )\n",
    "    t0=time.time()\n",
    "    pinn.fit(X_train, y_train)\n",
    "    dt=time.time()-t0\n",
    "\n",
    "    # save training loss\n",
    "    loss_csv = plots_data_dir / \"training\" / \"pinn_loss_history.csv\"\n",
    "    loss_png = plots_dir / \"training\" / \"pinn_loss_history.png\"\n",
    "    epochs = np.arange(1, len(pinn.loss_history)+1)\n",
    "    pd.DataFrame({\"epoch\": epochs, \"loss\": pinn.loss_history}).to_csv(loss_csv, index=False)\n",
    "    plt.figure(); plt.plot(epochs, pinn.loss_history); plt.xlabel(\"Epoch\"); plt.ylabel(\"Training loss\")\n",
    "    plt.title(\"PINN training loss vs epoch\"); plt.tight_layout(); plt.savefig(loss_png, dpi=180); _maybe_show()\n",
    "\n",
    "    if len(pinn.loss_history) >= 10:\n",
    "        k = 25\n",
    "        kernel = np.ones(k)/k\n",
    "        smooth = np.convolve(pinn.loss_history, kernel, mode=\"valid\")\n",
    "        plt.figure(); plt.plot(np.arange(k, k+len(smooth)), smooth)\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Training loss (moving avg)\")\n",
    "        plt.title(f\"PINN training loss (window={k})\")\n",
    "        plt.tight_layout(); plt.savefig(plots_dir / \"training\" / \"pinn_loss_history_smooth.png\", dpi=180); _maybe_show()\n",
    "\n",
    "    print(\"[5/5] Evaluating… (grouped split)\")\n",
    "    y_pred = pinn.predict(X_test)\n",
    "    metrics = evaluate_predictions(y_test, y_pred, targ_cols)\n",
    "    pd.DataFrame([metrics|{\"train_seconds\":dt}]).to_csv(Path(CONFIG[\"paths\"][\"output_dir\"]) / \"metrics_test.csv\", index=False)\n",
    "    print(\"   Test metrics:\", metrics)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"concentration_mM\": X_test[:,0], \"temperature_C\": X_test[:,1], \"frequency_Hz\": X_test[:,2],\n",
    "        \"Z_real_true\": y_test[:,0], \"Z_imag_neg_true\": y_test[:,1],\n",
    "        \"Z_real_pred\": y_pred[:,0], \"Z_imag_neg_pred\": y_pred[:,1]\n",
    "    }).to_csv(Path(CONFIG[\"paths\"][\"output_dir\"]) / \"test_predictions_pinn.csv\", index=False)\n",
    "\n",
    "    for j, tname in enumerate(targ_cols):\n",
    "        parity_plot_and_csv(y_test[:,j], y_pred[:,j], tname,\n",
    "            img_path = plots_dir / \"parity\" / f\"pinn_parity_{tname}.png\",\n",
    "            csv_path = plots_data_dir / \"parity\" / f\"pinn_parity_{tname}.csv\")\n",
    "        residual_plot_and_csv(y_test[:,j], y_pred[:,j], tname,\n",
    "            img_path = plots_dir / \"residuals\" / f\"pinn_residual_{tname}.png\",\n",
    "            csv_path = plots_data_dir / \"residuals\" / f\"pinn_residual_{tname}.csv\")\n",
    "        error_hist_plot_and_csv(y_test[:,j], y_pred[:,j], tname,\n",
    "            img_path = plots_dir / \"hist\" / f\"pinn_errhist_{tname}.png\",\n",
    "            bins = 30,\n",
    "            hist_csv_path = plots_data_dir / \"hist\" / f\"pinn_errhist_{tname}_bins.csv\",\n",
    "            fit_csv_path  = plots_data_dir / \"hist\" / f\"pinn_errhist_{tname}_fit.csv\")\n",
    "\n",
    "    # Save model (also store y-normalization to enable inverse at inference)\n",
    "    torch.save({\"state_dict\": pinn._net.state_dict(), \"xmu\": pinn._xmu, \"xstd\": pinn._xstd,\n",
    "                \"train_config\": CONFIG[\"pinn\"][\"train\"], \"y_norm\": pinn._y_norm,\n",
    "                \"teacher_used\": (pinn.teacher_df is not None and pinn.teacher_use)},\n",
    "               Path(CONFIG[\"paths\"][\"output_dir\"]) / \"pinn_model.pt\")\n",
    "    return pinn, (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# --------------------------\n",
    "# Extrapolation + Heatmaps\n",
    "# --------------------------\n",
    "def evaluate_extrapolation(pinn, min_c: float, max_c: float, tag: str = None):\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"]).resolve()\n",
    "    base = out_dir / \"extrapolation_range\" / (tag or f\"conc{min_c:g}_{max_c:g}\")\n",
    "    (base / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "    (base / \"plots_data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tf = CONFIG[\"data\"][\"train_filters\"].copy()\n",
    "    CONFIG[\"data\"][\"train_filters\"] = {\"conc_mM_min\": None, \"conc_mM_max\": None, \"temp_C_min\": None, \"temp_C_max\": None}\n",
    "    try:\n",
    "        data_all = build_dataset(Path(CONFIG[\"paths\"][\"input_root\"]))\n",
    "    finally:\n",
    "        CONFIG[\"data\"][\"train_filters\"] = tf\n",
    "\n",
    "    m = (data_all[\"concentration_mM\"] >= float(min_c)) & (data_all[\"concentration_mM\"] <= float(max_c))\n",
    "    eval_df = data_all.loc[m].copy()\n",
    "\n",
    "    with open(base / \"selection_info.txt\", \"w\") as fh:\n",
    "        fh.write(f\"Found rows in [{min_c},{max_c}] mM: {len(eval_df)}\\n\")\n",
    "\n",
    "    if eval_df.empty:\n",
    "        print(f\"No rows found in {min_c}-{max_c} mM.\"); return\n",
    "\n",
    "    feat_cols = [\"concentration_mM\",\"temperature_C\",\"frequency_Hz\"]\n",
    "    targ_cols = CONFIG[\"targets\"][\"target_columns\"]\n",
    "    X_eval = eval_df[feat_cols].astype(float).values\n",
    "    y_true = eval_df[targ_cols].astype(float).values\n",
    "    y_pred = pinn.predict(X_eval)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"concentration_mM\": eval_df[\"concentration_mM\"].values,\n",
    "        \"temperature_C\": eval_df[\"temperature_C\"].values,\n",
    "        \"frequency_Hz\": eval_df[\"frequency_Hz\"].values,\n",
    "        \"Z_real_true\": y_true[:,0],\n",
    "        \"Z_imag_neg_true\": y_true[:,1],\n",
    "        \"Z_real_pred\": y_pred[:,0],\n",
    "        \"Z_imag_neg_pred\": y_pred[:,1],\n",
    "    }).to_csv(base / \"predictions_pinn.csv\", index=False)\n",
    "\n",
    "    metrics = evaluate_predictions(y_true, y_pred, targ_cols)\n",
    "    pd.DataFrame([metrics]).to_csv(base / \"metrics_pinn.csv\", index=False)\n",
    "    print(f\"Extrapolation rows: {len(eval_df)}  ->  metrics: {metrics}\")\n",
    "\n",
    "    for j, tname in enumerate(targ_cols):\n",
    "        parity_plot_and_csv(y_true[:, j], y_pred[:, j], tname,\n",
    "            img_path = base / \"plots\" / f\"pinn_parity_{tname}.png\",\n",
    "            csv_path = base / \"plots_data\" / f\"pinn_parity_{tname}.csv\")\n",
    "        residual_plot_and_csv(y_true[:, j], y_pred[:, j], tname,\n",
    "            img_path = base / \"plots\" / f\"pinn_residual_{tname}.png\",\n",
    "            csv_path = base / \"plots_data\" / f\"pinn_residual_{tname}.csv\")\n",
    "        error_hist_plot_and_csv(y_true[:, j], y_pred[:, j], tname,\n",
    "            img_path = base / \"plots\" / f\"pinn_errhist_{tname}.png\",\n",
    "            bins = 30,\n",
    "            hist_csv_path = base / \"plots_data\" / f\"pinn_errhist_{tname}_bins.csv\",\n",
    "            fit_csv_path  = base / \"plots_data\" / f\"pinn_errhist_{tname}_fit.csv\")\n",
    "\n",
    "def visualize_theta_heatmaps(pinn):\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"]).resolve()\n",
    "    heat_dir = out_dir / \"theta_heatmaps\"; heat_dir.mkdir(parents=True, exist_ok=True)\n",
    "    heat_csv = out_dir / \"theta_heatmaps_csv\"; heat_csv.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    H = CONFIG[\"heatmap\"]\n",
    "    C_vals = np.linspace(H[\"C_min\"], H[\"C_max\"], H[\"C_points\"])\n",
    "    T_vals = np.linspace(H[\"T_min\"], H[\"T_max\"], H[\"T_points\"])\n",
    "    Cg, Tg = np.meshgrid(C_vals, T_vals, indexing=\"xy\")\n",
    "\n",
    "    grids = pinn.predict_theta(Cg, Tg)\n",
    "\n",
    "    df_long = pd.DataFrame({\n",
    "        \"concentration_mM\": np.repeat(C_vals, T_vals.size),\n",
    "        \"temperature_C\":    np.tile(T_vals, C_vals.size),\n",
    "        \"Rs\":   grids[\"Rs\"].T.ravel(), \"Rp\": grids[\"Rp\"].T.ravel(),\n",
    "        \"Y0\":   grids[\"Y0\"].T.ravel(), \"n0\": grids[\"n0\"].T.ravel(),\n",
    "        \"r\":    grids[\"r\"].T.ravel(),  \"y0\": grids[\"y0\"].T.ravel(),\n",
    "        \"n1\":   grids[\"n1\"].T.ravel(), \"L\":  grids[\"L\"].T.ravel(),\n",
    "    })\n",
    "    df_long.to_csv(heat_csv / \"theta_grid_long.csv\", index=False)\n",
    "\n",
    "    def _heat(A, title, fname, cmap=\"viridis\"):\n",
    "        plt.figure(figsize=(6,4.8))\n",
    "        im = plt.imshow(A, origin=\"lower\", aspect=\"auto\",\n",
    "                        extent=[C_vals.min(), C_vals.max(), T_vals.min(), T_vals.max()], cmap=cmap)\n",
    "        cb = plt.colorbar(im); cb.set_label(title)\n",
    "        plt.xlabel(\"Concentration (mM)\"); plt.ylabel(\"Temperature (°C)\")\n",
    "        plt.title(f\"{title} vs (C,T)\")\n",
    "        plt.tight_layout(); plt.savefig(heat_dir / fname, dpi=200); _maybe_show()\n",
    "\n",
    "    _heat(grids[\"Rs\"], \"Rs (Ω)\", \"theta_Rs.png\")\n",
    "    _heat(grids[\"Rp\"], \"Rp (Ω)\", \"theta_Rp.png\")\n",
    "    _heat(grids[\"Y0\"], \"Y0 (Ω⁻¹ sⁿ⁰)\", \"theta_Y0.png\")\n",
    "    _heat(grids[\"n0\"], \"n0 (–)\", \"theta_n0.png\")\n",
    "    _heat(grids[\"r\"],  \"r (Ω/len)\", \"theta_r.png\")\n",
    "    _heat(grids[\"y0\"], \"y0 (Ω⁻¹ sⁿ¹/len)\", \"theta_y0.png\")\n",
    "    _heat(grids[\"n1\"], \"n1 (–)\", \"theta_n1.png\")\n",
    "    _heat(grids[\"L\"],  \"L (len)\", \"theta_L.png\")\n",
    "\n",
    "    print(f\"Saved θ(C,T) heatmaps -> {heat_dir}\")\n",
    "    print(f\"Saved θ(C,T) grid CSV -> {heat_csv / 'theta_grid_long.csv'}\")\n",
    "\n",
    "# ==========================\n",
    "# Run (train → extrapolate → heatmaps)\n",
    "# ==========================\n",
    "pinn, _ = train_pinn_and_report()\n",
    "\n",
    "lo, hi = CONFIG[\"extrapolation\"][\"range_conc_mM\"]\n",
    "evaluate_extrapolation(pinn, lo, hi, tag=CONFIG[\"extrapolation\"][\"tag\"])\n",
    "\n",
    "visualize_theta_heatmaps(pinn)\n",
    "\n",
    "plt.close(\"all\"); gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
