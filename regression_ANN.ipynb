{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EIS Single-Frequency ML Pipeline — v5 + Group-Safe Split + Flexible Z + Group-aware CV & Learning Curves\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Changes vs previous cell:\n",
    "# - RandomizedSearchCV now uses GroupKFold and fits with groups=...\n",
    "# - Learning curves use GroupKFold and groups=...\n",
    "# - Optional rounding for (C,T) groups: CONFIG[\"split\"][\"group_round_C\"/\"group_round_T\"]\n",
    "\n",
    "import os, re, json, math, random, warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams[\"figure.dpi\"] = 130\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, RandomizedSearchCV, learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.base import clone, BaseEstimator, RegressorMixin\n",
    "\n",
    "from sklearn.linear_model import Ridge, ElasticNet, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, RationalQuadratic, DotProduct, WhiteKernel, ConstantKernel as C\n",
    "\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optional symbolic regression backends (auto-disabled if not installed)\n",
    "_HAVE_PYSR = False\n",
    "_HAVE_GPLEARN = False\n",
    "try:\n",
    "    from pysr import PySRRegressor\n",
    "    _HAVE_PYSR = True\n",
    "except Exception:\n",
    "    try:\n",
    "        from gplearn.genetic import SymbolicRegressor as GPLearnSR\n",
    "        _HAVE_GPLEARN = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def arrhenius_features(X):\n",
    "    \"\"\"Input X columns: [concentration_mM, temperature_C, frequency_Hz]\"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    C   = X[:, 0]\n",
    "    T_C = X[:, 1]\n",
    "    f   = X[:, 2]\n",
    "\n",
    "    T_K     = T_C + 273.15\n",
    "    inv_TK  = 1.0 / T_K\n",
    "    logC    = np.log(np.clip(C, 1e-9, None))\n",
    "    CxInvT  = C * inv_TK\n",
    "\n",
    "    return np.column_stack([C, T_C, f, T_K, inv_TK, logC, CxInvT])\n",
    "\n",
    "# =====================\n",
    "# ====== CONFIG =======\n",
    "# =====================\n",
    "CONFIG: Dict = {\n",
    "    \"paths\": {\n",
    "        \"input_root\": \"/Users/hosseinostovar/Desktop/BACKUP/Data/single_frequency_collected\",\n",
    "        \"output_dir\": \"/Users/hosseinostovar/Desktop/BACKUP/Data/ml_outputs_singleFrequency_v5_groupsafe_flexZ_groupCV\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        # Flexible header matching. Add your new header names here if needed.\n",
    "        \"column_aliases\": {\n",
    "            \"frequency\": [\n",
    "                \"Frequency (Hz)\", \"frequency (hz)\", \"freq (hz)\", \"frequency\", \"f (hz)\"\n",
    "            ],\n",
    "            \"z_real\": [\n",
    "                \"Z' (Ω)\", \"Z' (ohm)\", \"z' (ω)\", \"z_real (Ω)\", \"zreal (Ω)\", \"re(z) (Ω)\", \"re(z)\"\n",
    "            ],\n",
    "            # Already -Im(Z):\n",
    "            \"z_imag_neg\": [\n",
    "                \"-Z'' (Ω)\", \"-Z'' (ohm)\", \"-z'' (ω)\", \"-z_imag (Ω)\", \"-im(z) (Ω)\", \"-imag (Ω)\"\n",
    "            ],\n",
    "            # +Im(Z) (we will flip sign):\n",
    "            \"z_imag_pos\": [\n",
    "                \"Z'' (Ω)\", \"Z'' (ohm)\", \"z'' (ω)\", \"z_imag (Ω)\", \"im(z) (Ω)\", \"imag (Ω)\"\n",
    "            ],\n",
    "        },\n",
    "        # Legacy exacts (used only if aliases fail)\n",
    "        \"col_frequency\": \"Frequency (Hz)\",\n",
    "        \"col_z_real\":    \"Z' (Ω)\",\n",
    "        \"col_z_imag_neg\":\"-Z'' (Ω)\",\n",
    "\n",
    "        \"frequency_filter_hz\": None,   # e.g., 1000.0 for single-frequency training\n",
    "        \"accept_nested_summary\": True,\n",
    "        \"accept_flat_collected\": True,\n",
    "        \"read_csv_kwargs\": {\"encoding\": \"utf-8\"},\n",
    "\n",
    "        # Optional training-domain filter\n",
    "        \"train_filters\": {\n",
    "            \"conc_mM_min\": 5.0, \"conc_mM_max\": 15.0,\n",
    "            \"temp_C_min\": None, \"temp_C_max\": None\n",
    "        },\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"use_features\": [\"concentration_mM\", \"temperature_C\", \"frequency_Hz\"],\n",
    "        \"poly_degrees\": [2],\n",
    "    },\n",
    "    \"feature_engineering\": {\"arrhenius\": {\"use\": True}},\n",
    "    \"targets\": {\"target_columns\": [\"Z_real\", \"Z_imag_neg\"]},\n",
    "    \"normalization\": { \"targets\": {\"enabled\": False, \"method\": \"standard\"} },\n",
    "    # NEW: group rounding for (C,T)\n",
    "    \"split\": {\"test_size\": 0.2, \"random_state\": 42, \"group_round_C\": 0, \"group_round_T\": 0},\n",
    "    # CV budget\n",
    "    \"cv\": {\"n_splits\": 5, \"n_iter\": 25, \"n_jobs\": -1, \"verbose\": 1},\n",
    "    \"progress\": {\"enable_bars\": True, \"show_file_scan\": True},\n",
    "    \"plots\": {\n",
    "        \"save_all_models\": True,\n",
    "        \"top_k_models_to_plot\": 3,\n",
    "        \"make_learning_curve\": True,\n",
    "        \"learning_curve_train_sizes\": [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        \"show_inline\": False\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"ridge\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", Ridge())]),\n",
    "            \"param_distributions\": {\"reg__alpha\": np.logspace(-3, 3, 50)},\n",
    "        },\n",
    "        \"elasticnet\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", ElasticNet(max_iter=8000))]),\n",
    "            \"param_distributions\": {\n",
    "                \"reg__alpha\": np.logspace(-4, 2, 50),\n",
    "                \"reg__l1_ratio\": np.linspace(0.05, 0.95, 19),\n",
    "            },\n",
    "        },\n",
    "        \"pls\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler(with_mean=True, with_std=True)), (\"reg\", PLSRegression())]),\n",
    "            \"param_distributions\": {\"reg__n_components\": list(range(1, 10))},\n",
    "        },\n",
    "        \"poly_ridge\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline_template\": Pipeline([(\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                                           (\"scaler\", StandardScaler()),\n",
    "                                           (\"reg\", Ridge())]),\n",
    "            \"param_distributions\": {\"reg__alpha\": np.logspace(-3, 3, 50)},\n",
    "        },\n",
    "        \"huber\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", MultiOutputRegressor(HuberRegressor()))]),\n",
    "            \"param_distributions\": {\n",
    "                \"reg__estimator__epsilon\": [1.1, 1.35, 1.5, 1.75],\n",
    "                \"reg__estimator__alpha\": np.logspace(-6, -2, 5)\n",
    "            },\n",
    "        },\n",
    "        \"theilsen\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"reg\", MultiOutputRegressor(\n",
    "                    TheilSenRegressor(random_state=42, max_subpopulation=1e4)\n",
    "                ))\n",
    "            ]),\n",
    "            \"param_distributions\": {\n",
    "                \"reg__estimator__max_subpopulation\": [1e4, 2e4, 5e4],\n",
    "                \"reg__estimator__max_iter\": [200, 500, 1000],\n",
    "                \"reg__estimator__tol\": np.logspace(-4, -2, 3),\n",
    "            },\n",
    "        },\n",
    "        \"arrhenius_poly_ridge\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([\n",
    "                (\"fe\", FunctionTransformer(arrhenius_features, validate=False, feature_names_out='one-to-one')),\n",
    "                (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"reg\", Ridge())\n",
    "            ]),\n",
    "            \"param_distributions\": {\"reg__alpha\": np.logspace(-3, 3, 50)},\n",
    "        },\n",
    "        \"svr_rbf\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", MultiOutputRegressor(SVR(kernel=\"rbf\")))]),\n",
    "            \"param_distributions\": {\n",
    "                \"reg__estimator__C\": np.logspace(-2, 3, 15),\n",
    "                \"reg__estimator__epsilon\": np.logspace(-3, -0.3, 8),\n",
    "                \"reg__estimator__gamma\": [\"scale\", \"auto\"] + list(np.logspace(-4, 1, 8)),\n",
    "            },\n",
    "        },\n",
    "        \"kernel_ridge_rbf\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", KernelRidge(kernel=\"rbf\"))]),\n",
    "            \"param_distributions\": {\"reg__alpha\": np.logspace(-4, 1, 10), \"reg__gamma\": np.logspace(-4, 1, 10)},\n",
    "        },\n",
    "        \"mlp_ann\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": Pipeline([(\"scaler\", StandardScaler()), (\"reg\", MLPRegressor(max_iter=4000, random_state=42))]),\n",
    "            \"param_distributions\": {\n",
    "                \"reg__hidden_layer_sizes\": [(64,), (128,), (64,64), (128,64), (128,128)],\n",
    "                \"reg__alpha\": np.logspace(-6, -2, 5),\n",
    "                \"reg__activation\": [\"relu\", \"tanh\"],\n",
    "                \"reg__learning_rate_init\": np.logspace(-4, -2, 3),\n",
    "            },\n",
    "        },\n",
    "        \"gpr\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": MultiOutputRegressor(GaussianProcessRegressor(\n",
    "                kernel=(C(1.0, (1e-3, 1e3)) * RationalQuadratic(alpha=1.0, length_scale=10.0,\n",
    "                        alpha_bounds=(1e-3, 1e3), length_scale_bounds=(1e-2, 1e3))\n",
    "                        + C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0, length_scale_bounds=(1e-2, 1e3))\n",
    "                        + DotProduct())\n",
    "                + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-8, 1e-1)),\n",
    "                normalize_y=True, random_state=42\n",
    "            )),\n",
    "            \"param_distributions\": {\n",
    "                \"estimator__alpha\": [1e-10, 1e-6, 1e-4, 1e-3],\n",
    "                \"estimator__normalize_y\": [True, False],\n",
    "            },\n",
    "        },\n",
    "        \"random_forest\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": RandomForestRegressor(random_state=42),\n",
    "            \"param_distributions\": {\n",
    "                \"n_estimators\": list(range(200, 801, 100)),\n",
    "                \"max_depth\": [None] + list(range(5, 41, 5)),\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "                \"max_features\": [\"sqrt\", \"log2\"],\n",
    "            },\n",
    "        },\n",
    "        \"extra_trees\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": ExtraTreesRegressor(random_state=42),\n",
    "            \"param_distributions\": {\n",
    "                \"n_estimators\": list(range(300, 1001, 100)),\n",
    "                \"max_depth\": [None] + list(range(5, 51, 5)),\n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "                \"max_features\": [\"sqrt\", \"log2\"],\n",
    "                \"bootstrap\": [False],\n",
    "            },\n",
    "        },\n",
    "        \"gbr\": {\n",
    "            \"enabled\": True,\n",
    "            \"pipeline\": MultiOutputRegressor(GradientBoostingRegressor(random_state=42)),\n",
    "            \"param_distributions\": {\n",
    "                \"estimator__n_estimators\": list(range(200, 801, 100)),\n",
    "                \"estimator__learning_rate\": np.logspace(-3, -0.3, 10),\n",
    "                \"estimator__max_depth\": list(range(2, 9)),\n",
    "                \"estimator__subsample\": [0.7, 0.85, 1.0],\n",
    "            },\n",
    "        },\n",
    "        \"symbolic\": {\"enabled\": True, \"backend\": \"auto\", \"n_iter\": 200},\n",
    "    },\n",
    "    \"outputs\": {\"save_all_model_artifacts\": True}\n",
    "}\n",
    "\n",
    "if not (_HAVE_PYSR or _HAVE_GPLEARN):\n",
    "    CONFIG[\"models\"][\"symbolic\"][\"enabled\"] = False\n",
    "\n",
    "# =====================\n",
    "# ===== UTILITIES =====\n",
    "# =====================\n",
    "CONC_RE = re.compile(r'(?P<val>[-+]?[0-9]*\\.?[0-9]+)\\s*(?P<unit>mM|M|uM|µM)', re.IGNORECASE)\n",
    "TEMP_RE = re.compile(r'(?P<val>[-+]?[0-9]*\\.?[0-9]+)\\s*[cC]')\n",
    "UNIT_SCALE = {\"M\": 1000.0, \"mM\": 1.0, \"uM\": 0.001, \"µM\": 0.001}\n",
    "\n",
    "def parse_concentration_to_mM(text: str) -> float:\n",
    "    if not isinstance(text, str): return np.nan\n",
    "    m = CONC_RE.search(text)\n",
    "    if not m: return np.nan\n",
    "    val = float(m.group(\"val\")); unit = m.group(\"unit\")\n",
    "    return val * UNIT_SCALE.get(unit, 1.0)\n",
    "\n",
    "def parse_temperature_to_C(text: str) -> float:\n",
    "    if not isinstance(text, str): return np.nan\n",
    "    m = TEMP_RE.search(text)\n",
    "    if not m: return np.nan\n",
    "    return float(m.group(\"val\"))\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    return re.sub(r\"[\\s\\-\\_\\(\\)\\[\\]\\{\\}\\|'\\\"°Ωohmω]\", \"\", s.strip().lower())\n",
    "\n",
    "def _find_col_by_aliases(df: pd.DataFrame, aliases: List[str]) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    low_map = {c.lower(): c for c in cols}\n",
    "    for a in aliases:\n",
    "        if a.lower() in low_map:\n",
    "            return low_map[a.lower()]\n",
    "    norm_map = {_normalize(c): c for c in cols}\n",
    "    for a in aliases:\n",
    "        na = _normalize(a)\n",
    "        if na in norm_map:\n",
    "            return norm_map[na]\n",
    "    return None\n",
    "\n",
    "def resolve_eis_columns(df: pd.DataFrame) -> tuple[str, str, pd.Series]:\n",
    "    \"\"\"Return (freq_col_name, z_real_col_name, z_imag_neg_series) with -Im(Z) convention.\"\"\"\n",
    "    al = CONFIG[\"data\"][\"column_aliases\"]\n",
    "    f_col     = _find_col_by_aliases(df, al[\"frequency\"])\n",
    "    zr_col    = _find_col_by_aliases(df, al[\"z_real\"])\n",
    "    zi_neg    = _find_col_by_aliases(df, al[\"z_imag_neg\"])\n",
    "    zi_pos    = _find_col_by_aliases(df, al[\"z_imag_pos\"])\n",
    "    if f_col is None and CONFIG[\"data\"][\"col_frequency\"] in df.columns:\n",
    "        f_col = CONFIG[\"data\"][\"col_frequency\"]\n",
    "    if zr_col is None and CONFIG[\"data\"][\"col_z_real\"] in df.columns:\n",
    "        zr_col = CONFIG[\"data\"][\"col_z_real\"]\n",
    "    if zi_neg is None and CONFIG[\"data\"][\"col_z_imag_neg\"] in df.columns:\n",
    "        zi_neg = CONFIG[\"data\"][\"col_z_imag_neg\"]\n",
    "    if f_col is None or zr_col is None or (zi_neg is None and zi_pos is None):\n",
    "        raise KeyError(f\"Could not resolve columns. Found={list(df.columns)}\")\n",
    "    if zi_neg is not None:\n",
    "        zi_series = pd.to_numeric(df[zi_neg], errors=\"coerce\")\n",
    "    else:\n",
    "        zi_series = -pd.to_numeric(df[zi_pos], errors=\"coerce\")\n",
    "    return f_col, zr_col, zi_series\n",
    "\n",
    "def discover_files(root: Path) -> List[Tuple[Path, str, str]]:\n",
    "    discovered = []\n",
    "    if CONFIG[\"data\"][\"accept_nested_summary\"]:\n",
    "        for p in root.rglob(\"single_frequency_summary.csv\"):\n",
    "            try:\n",
    "                temperature_str = p.parent.name\n",
    "                concentration_str = p.parent.parent.name\n",
    "                discovered.append((p, concentration_str, temperature_str))\n",
    "            except Exception:\n",
    "                continue\n",
    "    if CONFIG[\"data\"][\"accept_flat_collected\"]:\n",
    "        for p in root.rglob(\"single_frequency_*_*.csv\"):\n",
    "            name = p.name\n",
    "            if name == \"single_frequency_summary.csv\" or not name.endswith(\".csv\"): continue\n",
    "            core = name.replace(\"single_frequency_\", \"\").replace(\".csv\", \"\")\n",
    "            parts = core.split(\"_\")\n",
    "            if len(parts) >= 2:\n",
    "                concentration_str = parts[0]; temperature_str = parts[1]\n",
    "                discovered.append((p, concentration_str, temperature_str))\n",
    "    unique = []; seen = set()\n",
    "    for item in discovered:\n",
    "        key = (str(item[0]), item[1], item[2])\n",
    "        if key not in seen:\n",
    "            seen.add(key); unique.append(item)\n",
    "    return unique\n",
    "\n",
    "def load_one_csv(path: Path) -> pd.DataFrame:\n",
    "    kwargs = CONFIG[\"data\"][\"read_csv_kwargs\"]\n",
    "    return pd.read_csv(path, **kwargs)\n",
    "\n",
    "def build_dataset(root: Path) -> pd.DataFrame:\n",
    "    files = discover_files(root)\n",
    "    if not files: raise FileNotFoundError(f\"No CSV files found under: {root}\")\n",
    "    rows = []\n",
    "    freq_filter = CONFIG[\"data\"][\"frequency_filter_hz\"]\n",
    "\n",
    "    it = tqdm(files, desc=\"Reading CSV files\", unit=\"file\") if CONFIG[\"progress\"][\"show_file_scan\"] else files\n",
    "    for csv_path, conc_str, temp_str in it:\n",
    "        df = load_one_csv(csv_path)\n",
    "        try:\n",
    "            f_col, zr_col, zi_neg_series = resolve_eis_columns(df)\n",
    "        except Exception:\n",
    "            continue\n",
    "        tmp = pd.DataFrame({\n",
    "            \"frequency_Hz\": pd.to_numeric(df[f_col], errors=\"coerce\"),\n",
    "            \"Z_real\":       pd.to_numeric(df[zr_col], errors=\"coerce\"),\n",
    "            \"Z_imag_neg\":   pd.to_numeric(zi_neg_series, errors=\"coerce\"),\n",
    "        })\n",
    "        if freq_filter is not None:\n",
    "            tmp = tmp.loc[np.isclose(tmp[\"frequency_Hz\"].astype(float), float(freq_filter))]\n",
    "        if tmp.empty:\n",
    "            continue\n",
    "        tmp[\"concentration_mM\"] = parse_concentration_to_mM(conc_str)\n",
    "        tmp[\"temperature_C\"] = parse_temperature_to_C(temp_str)\n",
    "        tmp[\"source_file\"] = str(csv_path)\n",
    "        rows.append(tmp)\n",
    "\n",
    "    if not rows: raise RuntimeError(\"Discovered files, but none had the required columns.\")\n",
    "    data = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    for c in [\"concentration_mM\", \"temperature_C\", \"frequency_Hz\", \"Z_real\", \"Z_imag_neg\"]:\n",
    "        data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
    "    data = data.dropna(subset=[\"concentration_mM\", \"temperature_C\", \"frequency_Hz\", \"Z_real\", \"Z_imag_neg\"])\n",
    "\n",
    "    # Optional training-domain filter\n",
    "    fcfg = CONFIG[\"data\"][\"train_filters\"]\n",
    "    if any(v is not None for v in fcfg.values()):\n",
    "        m = pd.Series(True, index=data.index)\n",
    "        if fcfg[\"conc_mM_min\"] is not None: m &= data[\"concentration_mM\"] >= float(fcfg[\"conc_mM_min\"])\n",
    "        if fcfg[\"conc_mM_max\"] is not None: m &= data[\"concentration_mM\"] <= float(fcfg[\"conc_mM_max\"])\n",
    "        if fcfg[\"temp_C_min\"]  is not None: m &= data[\"temperature_C\"] >= float(fcfg[\"temp_C_min\"])\n",
    "        if fcfg[\"temp_C_max\"]  is not None: m &= data[\"temperature_C\"] <= float(fcfg[\"temp_C_max\"])\n",
    "        data = data.loc[m].copy()\n",
    "\n",
    "    return data\n",
    "\n",
    "# =====================\n",
    "# === Metrics & scorer\n",
    "# =====================\n",
    "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    r2s = []; maes = []; rmses = []\n",
    "    for j, name in enumerate([\"Z_real\", \"Z_imag_neg\"]):\n",
    "        r2 = r2_score(y_true[:, j], y_pred[:, j])\n",
    "        mae = mean_absolute_error(y_true[:, j], y_pred[:, j])\n",
    "        rmse = math.sqrt(mean_squared_error(y_true[:, j], y_pred[:, j]))\n",
    "        out[f\"R2_{name}\"] = r2; out[f\"MAE_{name}\"] = mae; out[f\"RMSE_{name}\"] = rmse\n",
    "        r2s.append(r2); maes.append(mae); rmses.append(rmse)\n",
    "    out[\"R2_mean\"] = float(np.mean(r2s))\n",
    "    out[\"MAE_mean\"] = float(np.mean(maes))\n",
    "    out[\"RMSE_mean\"] = float(np.mean(rmses))\n",
    "    return out\n",
    "\n",
    "def _rmse_mean(y_true, y_pred):\n",
    "    return float(np.mean([np.sqrt(np.mean((y_true[:, j] - y_pred[:, j])**2)) for j in range(y_true.shape[1])]))\n",
    "\n",
    "RMSE_MEAN_SCORER = make_scorer(_rmse_mean, greater_is_better=False)\n",
    "\n",
    "def to_serializable(obj):\n",
    "    if isinstance(obj, (str, int, float, bool)) or obj is None: return obj\n",
    "    if isinstance(obj, (np.integer, np.floating)): return obj.item()\n",
    "    if isinstance(obj, (list, tuple)): return [to_serializable(x) for x in obj]\n",
    "    if isinstance(obj, dict): return {str(k): to_serializable(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    if isinstance(obj, BaseEstimator): return obj.__class__.__name__\n",
    "    return repr(obj)\n",
    "\n",
    "# ==========================\n",
    "# === Target normalization\n",
    "# ==========================\n",
    "class YScaledRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimator, method=\"standard\"):\n",
    "        self.estimator = estimator\n",
    "        self.method = method\n",
    "        self._yscaler = None\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 1: y = y.reshape(-1, 1)\n",
    "        if self.method == \"standard\":\n",
    "            self._yscaler = StandardScaler()\n",
    "        elif self.method == \"minmax\":\n",
    "            self._yscaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown target normalization method: {self.method}\")\n",
    "        y_scaled = self._yscaler.fit_transform(y)\n",
    "        self.estimator.fit(X, y_scaled)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        y_scaled_pred = self.estimator.predict(X)\n",
    "        y_scaled_pred = np.asarray(y_scaled_pred)\n",
    "        if y_scaled_pred.ndim == 1: y_scaled_pred = y_scaled_pred.reshape(-1, 1)\n",
    "        return self._yscaler.inverse_transform(y_scaled_pred)\n",
    "\n",
    "def maybe_wrap_target_normalization(estimator, param_distributions):\n",
    "    norm_cfg = CONFIG.get(\"normalization\", {}).get(\"targets\", {})\n",
    "    if not norm_cfg.get(\"enabled\", False):\n",
    "        return estimator, param_distributions\n",
    "    wrapped = YScaledRegressor(estimator=estimator, method=norm_cfg.get(\"method\", \"standard\"))\n",
    "    remapped = {f\"estimator__{k}\": v for k, v in param_distributions.items()}\n",
    "    return wrapped, remapped\n",
    "\n",
    "# =====================\n",
    "# ===== PLOTTING ======\n",
    "# =====================\n",
    "def _maybe_show():\n",
    "    if CONFIG[\"plots\"][\"show_inline\"]:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def _ensure_plot_dirs(base: Path):\n",
    "    parity_dir = base / \"parity\"; parity_dir.mkdir(exist_ok=True, parents=True)\n",
    "    resid_dir  = base / \"residuals\"; resid_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hist_dir   = base / \"hist\"; hist_dir.mkdir(exist_ok=True, parents=True)\n",
    "    learn_dir  = base / \"learning\"; learn_dir.mkdir(exist_ok=True, parents=True)\n",
    "    split_dir  = base / \"split_info\"; split_dir.mkdir(exist_ok=True, parents=True)\n",
    "    return {\"parity\": parity_dir, \"residuals\": resid_dir, \"hist\": hist_dir, \"learning\": learn_dir, \"split\": split_dir}\n",
    "\n",
    "def _ensure_data_dirs(base: Path):\n",
    "    parity_dir = base / \"parity\"; parity_dir.mkdir(exist_ok=True, parents=True)\n",
    "    resid_dir  = base / \"residuals\"; resid_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hist_dir   = base / \"hist\"; hist_dir.mkdir(exist_ok=True, parents=True)\n",
    "    learn_dir  = base / \"learning\"; learn_dir.mkdir(exist_ok=True, parents=True)\n",
    "    split_dir  = base / \"split_info\"; split_dir.mkdir(exist_ok=True, parents=True)\n",
    "    return {\"parity\": parity_dir, \"residuals\": resid_dir, \"hist\": hist_dir, \"learning\": learn_dir, \"split\": split_dir}\n",
    "\n",
    "def parity_plot_and_csv(y_true, y_pred, target_name, img_path: Path, csv_path: Path):\n",
    "    pd.DataFrame({\"y_true\": y_true.ravel(), \"y_pred\": y_pred.ravel()}).to_csv(csv_path, index=False)\n",
    "    plt.figure()\n",
    "    plt.scatter(y_true, y_pred, s=12, alpha=0.7)\n",
    "    low = min(float(np.min(y_true)), float(np.min(y_pred)))\n",
    "    high = max(float(np.max(y_true)), float(np.max(y_pred)))\n",
    "    plt.plot([low, high], [low, high], linestyle=\"--\")\n",
    "    plt.xlabel(f\"True {target_name}\")\n",
    "    plt.ylabel(f\"Pred {target_name}\")\n",
    "    plt.title(f\"Parity: {target_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path, dpi=180)\n",
    "    _maybe_show()\n",
    "\n",
    "def residual_plot_and_csv(y_true, y_pred, target_name, img_path: Path, csv_path: Path):\n",
    "    residual = (y_pred - y_true).ravel()\n",
    "    pd.DataFrame({\"y_pred\": y_pred.ravel(), \"residual\": residual}).to_csv(csv_path, index=False)\n",
    "    plt.figure()\n",
    "    plt.scatter(y_pred, residual, s=12, alpha=0.7)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.xlabel(f\"Pred {target_name}\")\n",
    "    plt.ylabel(\"Residual (Pred - True)\")\n",
    "    plt.title(f\"Residuals: {target_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path, dpi=180)\n",
    "    _maybe_show()\n",
    "\n",
    "def _gauss_pdf(x, m, s):\n",
    "    if s <= 0: return np.zeros_like(x)\n",
    "    return (1.0 / (s * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - m)/s)**2)\n",
    "\n",
    "def error_hist_plot_and_csv(y_true, y_pred, target_name, img_path: Path, bins: int, hist_csv_path: Path, fit_csv_path: Path):\n",
    "    err = (y_pred - y_true).ravel()\n",
    "    counts, edges = np.histogram(err, bins=bins)\n",
    "    centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "    bin_w = float(edges[1] - edges[0]) if len(edges) > 1 else 1.0\n",
    "    mode_center = float(centers[np.argmax(counts)])\n",
    "\n",
    "    med = float(np.median(err))\n",
    "    mad = float(np.median(np.abs(err - med)))\n",
    "    sigma_rob = 1.4826 * mad if mad > 0 else float(np.std(err, ddof=1))\n",
    "\n",
    "    mean_bias = float(np.mean(err))\n",
    "    sigma_std = float(np.std(err, ddof=1)) if err.size > 1 else 0.0\n",
    "    skew = float(np.mean(((err - mean_bias) / (sigma_std if sigma_std>0 else 1))**3)) if sigma_std>0 else np.nan\n",
    "    kurt_excess = float(np.mean(((err - mean_bias) / (sigma_std if sigma_std>0 else 1))**4) - 3.0) if sigma_std>0 else np.nan\n",
    "\n",
    "    mu = mode_center\n",
    "    sigma = sigma_rob if sigma_rob > 0 else (sigma_std if sigma_std > 0 else 1e-9)\n",
    "    ci_low = mu - 1.96 * sigma; ci_high = mu + 1.96 * sigma\n",
    "\n",
    "    pd.DataFrame({\"bin_left\": edges[:-1], \"bin_right\": edges[1:], \"bin_center\": centers, \"count\": counts}).to_csv(hist_csv_path, index=False)\n",
    "    pd.DataFrame([{\n",
    "        \"mu_mode\": mu, \"sigma_robust\": sigma,\n",
    "        \"mean_bias\": mean_bias, \"sigma_std\": sigma_std,\n",
    "        \"ci95_low\": ci_low, \"ci95_high\": ci_high,\n",
    "        \"skew\": skew, \"kurtosis_excess\": kurt_excess,\n",
    "        \"n\": int(len(err)), \"bin_width\": bin_w\n",
    "    }]).to_csv(fit_csv_path, index=False)\n",
    "\n",
    "    x_grid = np.linspace(edges[0], edges[-1], 600)\n",
    "    gauss_counts_grid = len(err) * bin_w * _gauss_pdf(x_grid, mu, sigma)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(err, bins=bins)\n",
    "    plt.plot(x_grid, gauss_counts_grid, linewidth=2)\n",
    "    txt = (f\"Gaussian (mode-aligned)\\n\"\n",
    "           f\"μ≈{mu:.4g}, σ≈{sigma:.4g}, 95%≈[{ci_low:.4g},{ci_high:.4g}]\\n\"\n",
    "           f\"mean(bias)={mean_bias:.4g}, skew={skew:.3g}, kurt_ex={kurt_excess:.3g}\")\n",
    "    plt.text(0.02, 0.98, txt, transform=plt.gca().transAxes, va=\"top\", ha=\"left\",\n",
    "             bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"0.7\", alpha=0.9))\n",
    "    plt.xlabel(\"Error (Pred - True)\"); plt.ylabel(\"Count\"); plt.title(f\"Error Distribution: {target_name}\")\n",
    "    plt.tight_layout(); plt.savefig(img_path, dpi=180); _maybe_show()\n",
    "\n",
    "# --- Group-aware learning curve ---\n",
    "def plot_learning_and_csv_grouped(estimator, X, y, groups, img_path: Path, csv_path: Path, train_sizes):\n",
    "    groups = np.asarray(groups)\n",
    "    unique_groups = np.unique(groups)\n",
    "    n_groups = len(unique_groups)\n",
    "    n_splits_eff = max(2, min(CONFIG[\"cv\"][\"n_splits\"], n_groups))\n",
    "    cv = GroupKFold(n_splits=n_splits_eff)\n",
    "    ts, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        scoring=RMSE_MEAN_SCORER,\n",
    "        groups=groups,\n",
    "        n_jobs=CONFIG[\"cv\"][\"n_jobs\"]\n",
    "    )\n",
    "    train_rmse = -np.mean(train_scores, axis=1); train_std  =  np.std(train_scores, axis=1)\n",
    "    cv_rmse    = -np.mean(test_scores,  axis=1); cv_std     =  np.std(test_scores,  axis=1)\n",
    "\n",
    "    pd.DataFrame({\"train_size\": ts, \"train_rmse_mean\": train_rmse, \"train_rmse_std\": train_std,\n",
    "                  \"cv_rmse_mean\": cv_rmse, \"cv_rmse_std\": cv_std}).to_csv(csv_path, index=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ts, train_rmse, marker=\"o\", label=\"Train RMSE (mean)\")\n",
    "    plt.plot(ts, cv_rmse, marker=\"o\", label=\"CV RMSE (mean)\")\n",
    "    plt.xlabel(\"Training Samples\"); plt.ylabel(\"RMSE\"); plt.legend(); plt.title(\"Learning Curve (Group-aware)\")\n",
    "    plt.tight_layout(); plt.savefig(img_path, dpi=180); _maybe_show()\n",
    "\n",
    "# ============================\n",
    "# ===== Group utilities  =====\n",
    "# ============================\n",
    "def make_group_ids(df: pd.DataFrame, c_col=\"concentration_mM\", t_col=\"temperature_C\"):\n",
    "    rc = float(CONFIG[\"split\"].get(\"group_round_C\", 0) or 0)\n",
    "    rt = float(CONFIG[\"split\"].get(\"group_round_T\", 0) or 0)\n",
    "    C = pd.to_numeric(df[c_col], errors=\"coerce\").values\n",
    "    T = pd.to_numeric(df[t_col], errors=\"coerce\").values\n",
    "    if rc > 0: C = np.round(C/rc)*rc\n",
    "    if rt > 0: T = np.round(T/rt)*rt\n",
    "    return np.array([f\"{c:.12g}|{t:.12g}\" for c, t in zip(C, T)], dtype=object)\n",
    "\n",
    "# ============================\n",
    "# ===== Train / Predict ======\n",
    "# ============================\n",
    "# --- Group-aware randomized search ---\n",
    "def random_search_grouped(estimator, param_distributions: dict, X, y, groups):\n",
    "    groups = np.asarray(groups)\n",
    "    unique_groups = np.unique(groups)\n",
    "    n_groups = len(unique_groups)\n",
    "    n_splits_eff = max(2, min(CONFIG[\"cv\"][\"n_splits\"], n_groups))\n",
    "    cv = GroupKFold(n_splits=n_splits_eff)\n",
    "\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=CONFIG[\"cv\"][\"n_iter\"],\n",
    "        scoring=RMSE_MEAN_SCORER,\n",
    "        n_jobs=CONFIG[\"cv\"][\"n_jobs\"],\n",
    "        cv=cv,\n",
    "        random_state=CONFIG[\"split\"][\"random_state\"],\n",
    "        verbose=CONFIG[\"cv\"][\"verbose\"],\n",
    "        refit=True,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    rs.fit(X, y, groups=groups)   # <- crucial\n",
    "    return rs\n",
    "\n",
    "class SymbolicMultiOutput(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, backend=\"auto\", n_iter=200, random_state=42):\n",
    "        self.backend = backend\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.models_ = []\n",
    "    def _make_single(self):\n",
    "        if (self.backend == \"pysr\" or (self.backend == \"auto\" and _HAVE_PYSR)):\n",
    "            return PySRRegressor(\n",
    "                niterations=self.n_iter, maxsize=20,\n",
    "                unary_operators=[\"sin\", \"cos\", \"exp\", \"log\"],\n",
    "                binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "                loss=\"loss(x, y) = (x - y)^2\", random_state=self.random_state,\n",
    "                progress=False, verbosity=0,\n",
    "            )\n",
    "        elif (self.backend == \"gplearn\" or (self.backend == \"auto\" and _HAVE_GPLEARN)):\n",
    "            return GPLearnSR(\n",
    "                population_size=1000, generations=max(10, self.n_iter // 10),\n",
    "                tournament_size=20, stopping_criteria=0.0, metric=\"mse\",\n",
    "                parsimony_coefficient=0.001, random_state=self.random_state,\n",
    "                n_jobs=1, verbose=0,\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(\"No symbolic regression backend available.\")\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim == 1: y = y.reshape(-1, 1)\n",
    "        self.models_ = []\n",
    "        for j in range(y.shape[1]):\n",
    "            m = self._make_single()\n",
    "            m.fit(X, y[:, j])\n",
    "            self.models_.append(m)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for m in self.models_:\n",
    "            preds.append(np.asarray(m.predict(X)).reshape(-1, 1))\n",
    "        return np.hstack(preds)\n",
    "\n",
    "def train_all():\n",
    "    np.random.seed(CONFIG[\"split\"][\"random_state\"]); random.seed(CONFIG[\"split\"][\"random_state\"])\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"]); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plots_dir = out_dir / \"plots\"; plots_dir.mkdir(exist_ok=True)\n",
    "    plots_data_dir = out_dir / \"plots_data\"; plots_data_dir.mkdir(exist_ok=True)\n",
    "    models_dir = out_dir / \"models\"; models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    img_dirs = _ensure_plot_dirs(plots_dir)\n",
    "    csv_dirs = _ensure_data_dirs(plots_data_dir)\n",
    "\n",
    "    with open(out_dir / \"config_used.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(to_serializable(CONFIG), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"[1/7] Building dataset …\")\n",
    "    data = build_dataset(Path(CONFIG[\"paths\"][\"input_root\"]))\n",
    "    data.to_csv(out_dir / \"compiled_dataset.csv\", index=False)\n",
    "    print(f\"   -> {len(data)} rows\")\n",
    "\n",
    "    feature_cols = CONFIG[\"features\"][\"use_features\"]\n",
    "    target_cols  = CONFIG[\"targets\"][\"target_columns\"]\n",
    "    X = data[feature_cols].values\n",
    "    y = data[target_cols].values\n",
    "\n",
    "    # --- Group-safe train/test split (by (C,T)) ---\n",
    "    print(\"[2/7] Group-safe Train/Test split (by (C,T))\")\n",
    "    groups_all = make_group_ids(data, \"concentration_mM\", \"temperature_C\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG[\"split\"][\"test_size\"],\n",
    "                            random_state=CONFIG[\"split\"][\"random_state\"])\n",
    "    train_idx, test_idx = next(gss.split(X, y, groups=groups_all))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    groups_train, groups_test = groups_all[train_idx], groups_all[test_idx]\n",
    "\n",
    "    print(\"[3/7] Training models (group-aware CV) …\")\n",
    "    tasks = []\n",
    "    for name, spec in CONFIG[\"models\"].items():\n",
    "        if not spec.get(\"enabled\", False): continue\n",
    "        if name == \"symbolic\":\n",
    "            if not (_HAVE_PYSR or _HAVE_GPLEARN):\n",
    "                tqdm.write(\"   -> symbolic: skipped (no backend found)\")\n",
    "                continue\n",
    "            tasks.append((\"symbolic\", \"symbolic\", None))\n",
    "        elif \"pipeline_template\" in spec:\n",
    "            for deg in CONFIG[\"features\"][\"poly_degrees\"]:\n",
    "                tasks.append((f\"{name}_deg{deg}\", name, deg))\n",
    "        else:\n",
    "            tasks.append((name, name, None))\n",
    "\n",
    "    model_rows = []; all_searches = {}\n",
    "    bar = tqdm(total=len(tasks), desc=\"Model families\", unit=\"model\") if CONFIG[\"progress\"][\"enable_bars\"] else None\n",
    "    best_name = None; best_score = None\n",
    "\n",
    "    for label, name, deg in tasks:\n",
    "        if name == \"symbolic\":\n",
    "            est = SymbolicMultiOutput(backend=CONFIG[\"models\"][\"symbolic\"][\"backend\"],\n",
    "                                      n_iter=CONFIG[\"models\"][\"symbolic\"][\"n_iter\"],\n",
    "                                      random_state=CONFIG[\"split\"][\"random_state\"])\n",
    "            pipe = est\n",
    "            dist = {}\n",
    "            pipe.fit(X_train, y_train)\n",
    "            class DummySearch:\n",
    "                best_estimator_ = pipe\n",
    "                best_params_ = {\"backend\": (\"PySR\" if _HAVE_PYSR else (\"gplearn\" if _HAVE_GPLEARN else \"none\")),\n",
    "                                \"n_iter\": CONFIG[\"models\"][\"symbolic\"][\"n_iter\"]}\n",
    "                def predict(self, X): return pipe.predict(X)\n",
    "            search = DummySearch()\n",
    "        else:\n",
    "            if \"pipeline_template\" in CONFIG[\"models\"][name]:\n",
    "                pipe = clone(CONFIG[\"models\"][name][\"pipeline_template\"])\n",
    "                if deg is not None: pipe.set_params(poly__degree=deg)\n",
    "                dist = CONFIG[\"models\"][name][\"param_distributions\"]\n",
    "            else:\n",
    "                pipe = CONFIG[\"models\"][name][\"pipeline\"]\n",
    "                dist = CONFIG[\"models\"][name][\"param_distributions\"]\n",
    "            pipe, dist = maybe_wrap_target_normalization(pipe, dist)\n",
    "            search = random_search_grouped(pipe, dist, X_train, y_train, groups_train)\n",
    "\n",
    "        all_searches[label] = search\n",
    "\n",
    "        y_pred = search.predict(X_test)\n",
    "        metrics = evaluate_predictions(y_test, y_pred)\n",
    "        metrics[\"model\"] = label\n",
    "        metrics[\"best_params\"] = str(getattr(search, \"best_params_\", {}))\n",
    "        model_rows.append(metrics)\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"concentration_mM\": data.iloc[test_idx][\"concentration_mM\"].values,\n",
    "            \"temperature_C\":    data.iloc[test_idx][\"temperature_C\"].values,\n",
    "            \"frequency_Hz\":     data.iloc[test_idx][\"frequency_Hz\"].values,\n",
    "            \"Z_real_true\":      y_test[:,0], \"Z_imag_neg_true\": y_test[:,1],\n",
    "            \"Z_real_pred\":      y_pred[:,0], \"Z_imag_neg_pred\": y_pred[:,1]\n",
    "        }).to_csv(out_dir / f\"test_predictions_{label}.csv\", index=False)\n",
    "\n",
    "        if CONFIG[\"outputs\"][\"save_all_model_artifacts\"]:\n",
    "            joblib.dump(getattr(search, \"best_estimator_\", search), models_dir / f\"{label}.joblib\")\n",
    "\n",
    "        current_score = -metrics[\"RMSE_mean\"]\n",
    "        if (best_score is None) or (current_score > best_score):\n",
    "            best_score = current_score; best_name = label\n",
    "\n",
    "        if bar: bar.update(1)\n",
    "    if bar: bar.close()\n",
    "\n",
    "    print(\"[4/7] Writing report\")\n",
    "    report_df = pd.DataFrame(model_rows).sort_values(by=\"RMSE_mean\")\n",
    "    report_df.to_csv(out_dir / \"model_report.csv\", index=False)\n",
    "\n",
    "    best_search = all_searches[best_name]\n",
    "    best_model = getattr(best_search, \"best_estimator_\", best_search)\n",
    "    with open(out_dir / \"feature_columns.json\", \"w\", encoding=\"utf-8\") as f: json.dump(feature_cols, f)\n",
    "    with open(out_dir / \"target_columns.json\", \"w\", encoding=\"utf-8\") as f: json.dump(target_cols, f)\n",
    "    joblib.dump(best_model, out_dir / \"best_model.joblib\")\n",
    "    with open(out_dir / \"best_model_name.txt\", \"w\") as f: f.write(best_name)\n",
    "\n",
    "    print(\"[5/7] Plots + CSVs for models\")\n",
    "    if CONFIG[\"plots\"][\"save_all_models\"]:\n",
    "        models_to_plot = list(report_df[\"model\"])\n",
    "    else:\n",
    "        models_to_plot = list(report_df[\"model\"].head(CONFIG[\"plots\"][\"top_k_models_to_plot\"]))\n",
    "\n",
    "    for label in models_to_plot:\n",
    "        est = getattr(all_searches[label], \"best_estimator_\", all_searches[label])\n",
    "        y_pred = all_searches[label].predict(X_test)\n",
    "        for j, tname in enumerate(target_cols):\n",
    "            parity_plot_and_csv(y_test[:, j], y_pred[:, j], tname,\n",
    "                img_path = img_dirs[\"parity\"] / f\"{label}_parity_{tname}.png\",\n",
    "                csv_path = csv_dirs[\"parity\"] / f\"{label}_parity_{tname}.csv\")\n",
    "            residual_plot_and_csv(y_test[:, j], y_pred[:, j], tname,\n",
    "                img_path = img_dirs[\"residuals\"] / f\"{label}_residual_{tname}.png\",\n",
    "                csv_path = csv_dirs[\"residuals\"] / f\"{label}_residual_{tname}.csv\")\n",
    "            error_hist_plot_and_csv(y_test[:, j], y_pred[:, j], tname,\n",
    "                img_path = img_dirs[\"hist\"] / f\"{label}_errhist_{tname}.png\",\n",
    "                bins = 30,\n",
    "                hist_csv_path = csv_dirs[\"hist\"] / f\"{label}_errhist_{tname}_bins.csv\",\n",
    "                fit_csv_path  = csv_dirs[\"hist\"] / f\"{label}_errhist_{tname}_fit.csv\")\n",
    "        if CONFIG[\"plots\"][\"make_learning_curve\"]:\n",
    "            plot_learning_and_csv_grouped(est, X_train, y_train, groups_train,\n",
    "                img_path = img_dirs[\"learning\"] / f\"{label}_learning_curve.png\",\n",
    "                csv_path = csv_dirs[\"learning\"] / f\"{label}_learning_curve.csv\",\n",
    "                train_sizes = CONFIG[\"plots\"][\"learning_curve_train_sizes\"])\n",
    "\n",
    "    print(\"[6/7] Saving *best* model test-set predictions\")\n",
    "    y_best = all_searches[best_name].predict(X_test)\n",
    "    pd.DataFrame({\n",
    "        \"concentration_mM\": data.iloc[test_idx][\"concentration_mM\"].values,\n",
    "        \"temperature_C\":    data.iloc[test_idx][\"temperature_C\"].values,\n",
    "        \"frequency_Hz\":     data.iloc[test_idx][\"frequency_Hz\"].values,\n",
    "        \"Z_real_true\":      y_test[:,0], \"Z_imag_neg_true\": y_test[:,1],\n",
    "        \"Z_real_pred\":      y_best[:,0], \"Z_imag_neg_pred\": y_best[:,1]\n",
    "    }).to_csv(out_dir / \"test_predictions_best.csv\", index=False)\n",
    "\n",
    "    # Split diagnostics\n",
    "    ct_train = set(groups_train.tolist()); ct_test = set(groups_test.tolist())\n",
    "    overlap = ct_train.intersection(ct_test)\n",
    "    with open((plots_data_dir / \"split_info\" / \"ct_overlap_check.txt\"), \"w\") as fh:\n",
    "        fh.write(f\"unique (C,T) in train: {len(ct_train)}\\n\")\n",
    "        fh.write(f\"unique (C,T) in test:  {len(ct_test)}\\n\")\n",
    "        fh.write(f\"overlap count:         {len(overlap)}\\n\")\n",
    "\n",
    "    print(f\"[7/7] Done. Best model: {best_name}\")\n",
    "    print(f\"Outputs -> {out_dir.resolve()}\")\n",
    "\n",
    "def load_model_by_label(label: str):\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"]); models_dir = out_dir / \"models\"\n",
    "    model = joblib.load(models_dir / f\"{label}.joblib\")\n",
    "    feats = json.load(open(out_dir / \"feature_columns.json\"))\n",
    "    return model, feats\n",
    "\n",
    "def load_best_model():\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"])\n",
    "    model = joblib.load(out_dir / \"best_model.joblib\")\n",
    "    feats = json.load(open(out_dir / \"feature_columns.json\"))\n",
    "    return model, feats\n",
    "\n",
    "def predict_one_with(label: str, conc_mM: float, temp_C: float, freq_Hz: float):\n",
    "    model, feats = load_model_by_label(label)\n",
    "    X = np.array([[conc_mM, temp_C, freq_Hz]], dtype=float)\n",
    "    y = model.predict(X)\n",
    "    print(f\"[{label}] conc={conc_mM} mM, T={temp_C} °C, f={freq_Hz} Hz -> Z'={y[0,0]:.6g}, -Z''={y[0,1]:.6g}\")\n",
    "    return y[0,0], y[0,1]\n",
    "\n",
    "def evaluate_concentration_range_all_models(min_c: float, max_c: float, tag: str = None):\n",
    "    out_dir = Path(CONFIG[\"paths\"][\"output_dir\"])\n",
    "    model_list_path = out_dir / \"models\" / \"model_list.txt\"\n",
    "    if not model_list_path.exists():\n",
    "        print(\"No saved models found. Train first.\")\n",
    "        return\n",
    "    with open(model_list_path) as f:\n",
    "        labels = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "    original_filters = CONFIG[\"data\"][\"train_filters\"].copy()\n",
    "    CONFIG[\"data\"][\"train_filters\"] = {\"conc_mM_min\": None, \"conc_mM_max\": None, \"temp_C_min\": None, \"temp_C_max\": None}\n",
    "    try:\n",
    "        data_all = build_dataset(Path(CONFIG[\"paths\"][\"input_root\"]))\n",
    "    finally:\n",
    "        CONFIG[\"data\"][\"train_filters\"] = original_filters\n",
    "\n",
    "    m = (data_all[\"concentration_mM\"] >= float(min_c)) & (data_all[\"concentration_mM\"] <= float(max_c))\n",
    "    eval_df = data_all.loc[m].copy()\n",
    "    if eval_df.empty:\n",
    "        print(f\"No rows found in {min_c}-{max_c} mM.\")\n",
    "        return\n",
    "\n",
    "    feat_cols = CONFIG[\"features\"][\"use_features\"]\n",
    "    target_cols = CONFIG[\"targets\"][\"target_columns\"]\n",
    "    X_eval = eval_df[feat_cols].astype(float).values\n",
    "    y_true = eval_df[target_cols].astype(float).values\n",
    "\n",
    "    base = out_dir / \"extrapolation_range\" / (tag or f\"conc{min_c:g}_{max_c:g}\")\n",
    "    (base / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "    (base / \"plots_data\").mkdir(parents=True, exist_ok=True)\n",
    "    img_dirs = _ensure_plot_dirs(base / \"plots\")\n",
    "    csv_dirs = _ensure_data_dirs(base / \"plots_data\")\n",
    "\n",
    "    all_metrics = []\n",
    "    for label in labels:\n",
    "        try:\n",
    "            model, _ = load_model_by_label(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {label}: {e}\"); continue\n",
    "\n",
    "        y_pred = model.predict(X_eval)\n",
    "        pd.DataFrame({\n",
    "            \"concentration_mM\": eval_df[\"concentration_mM\"].values,\n",
    "            \"temperature_C\":    eval_df[\"temperature_C\"].values,\n",
    "            \"frequency_Hz\":     eval_df[\"frequency_Hz\"].values,\n",
    "            \"Z_real_true\":      y_true[:,0],\n",
    "            \"Z_imag_neg_true\":  y_true[:,1],\n",
    "            \"Z_real_pred\":      y_pred[:,0],\n",
    "            \"Z_imag_neg_pred\":  y_pred[:,1],\n",
    "        }).to_csv(base / f\"predictions_{label}.csv\", index=False)\n",
    "\n",
    "        metrics = evaluate_predictions(y_true, y_pred); metrics[\"model\"] = label\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "        for j, tname in enumerate(target_cols):\n",
    "            parity_plot_and_csv(y_true[:, j], y_pred[:, j], f\"{tname}\",\n",
    "                img_path = img_dirs[\"parity\"] / f\"{label}_parity_{tname}.png\",\n",
    "                csv_path = csv_dirs[\"parity\"] / f\"{label}_parity_{tname}.csv\")\n",
    "            residual_plot_and_csv(y_true[:, j], y_pred[:, j], f\"{tname}\",\n",
    "                img_path = img_dirs[\"residuals\"] / f\"{label}_residual_{tname}.png\",\n",
    "                csv_path = csv_dirs[\"residuals\"] / f\"{label}_residual_{tname}.csv\")\n",
    "            error_hist_plot_and_csv(y_true[:, j], y_pred[:, j], f\"{tname}\",\n",
    "                img_path = img_dirs[\"hist\"] / f\"{label}_errhist_{tname}.png\",\n",
    "                bins = 30,\n",
    "                hist_csv_path = csv_dirs[\"hist\"] / f\"{label}_errhist_{tname}_bins.csv\",\n",
    "                fit_csv_path  = csv_dirs[\"hist\"] / f\"{label}_errhist_{tname}_fit.csv\")\n",
    "\n",
    "    if all_metrics:\n",
    "        pd.DataFrame(all_metrics).sort_values(by=\"RMSE_mean\").to_csv(base / \"metrics_all_models.csv\", index=False)\n",
    "        print(f\"Saved extrapolation results for {len(all_metrics)} models -> {base.resolve()}\")\n",
    "\n",
    "# ==============================\n",
    "# ======= RUN TRAIN HERE =======\n",
    "# ==============================\n",
    "RUN_TRAIN = True\n",
    "if RUN_TRAIN:\n",
    "    train_all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
